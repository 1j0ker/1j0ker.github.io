<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-08-07T16:20:17.162Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>1j0ker</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python实用工具系列</title>
    <link href="http://yoursite.com/2019/08/07/python%E5%8E%BB%E9%87%8D%E5%88%A0%E9%99%A4/"/>
    <id>http://yoursite.com/2019/08/07/python去重删除/</id>
    <published>2019-08-07T15:02:17.000Z</published>
    <updated>2019-08-07T16:20:17.162Z</updated>
    
    <content type="html"><![CDATA[<h2>Python实现Excel表的去重、删除特定行\列</h2><p><em>最近分析导出数据的文件为excel表，就想用python来实现一下自动化操作，当然现在还是定制的Demo,还需要改进。</em><br><br></p><table><thead><tr><th align="center">第一列</th><th align="center">第二列</th><th align="center">第三列</th></tr></thead><tbody><tr><td align="center">4</td><td align="center">2</td><td align="center">3</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">删除此行</td><td align="center">删除</td><td align="center">此行</td></tr><tr><td align="center">5</td><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">4</td><td align="center">1</td><td align="center">3</td></tr><tr><td align="center">1</td><td align="center">3</td><td align="center">2</td></tr></tbody></table><p>Excel表如上；</p><h4>操作一、去重（duplicate）</h4><p>我这里选择了第三列，进行去重；<br>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd<span class="comment">#pandas模块处理表单的去重操作</span></span><br><span class="line"><span class="keyword">import</span> xlrd<span class="comment">#读excle表操作</span></span><br><span class="line"><span class="keyword">import</span> xlwt<span class="comment">#输出.xls文件，因为后面的删除行操作，我是引用的win32模块，所以，文件格式换成对应的xls比较方便</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Duplicate</span><span class="params">()</span>:</span></span><br><span class="line">    frame = pd.read_excel(<span class="string">r'你的xlsx文件名，填绝对路径'</span>)<span class="comment">#r注意非转译模式</span></span><br><span class="line">    data = pd.DataFrame(frame)</span><br><span class="line">    data.drop_duplicates([<span class="string">'第三列'</span>], keep=<span class="string">'first'</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># drop_duplicates用法：subset=‘需要去重复的列名’,keep=‘遇到重复的时保留第一个还是保留最后一个’,inplace=‘去除重复项，还是保留重复项的副本’</span></span><br><span class="line"></span><br><span class="line">    data.to_excel(<span class="string">r'你想保存到哪儿的.xls，绝对路径'</span>)<span class="comment">#.xls为后面的删除操作方便</span></span><br><span class="line">    print(<span class="string">'去重完成'</span>)</span><br></pre></td></tr></table></figure><p><strong>执行结果：</strong><br><br></p><table><thead><tr><th align="center">第一列</th><th align="center">第二列</th><th align="center">第三列</th></tr></thead><tbody><tr><td align="center">4</td><td align="center">2</td><td align="center">3</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">删除此行</td><td align="center">删除</td><td align="center">此行</td></tr><tr><td align="center">1</td><td align="center">3</td><td align="center">2</td></tr></tbody></table><h4>删除行操作</h4><p>这里我用的win32模块；<br>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> win32com.client</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">delete_row</span><span class="params">()</span>:</span></span><br><span class="line">    xlApp = win32com.client.Dispatch(<span class="string">'Excel.Application'</span>)<span class="comment">#创建excle项目</span></span><br><span class="line">    xlBook = xlApp.Workbooks.Open(<span class="string">r'你想操作的.xls文件，绝对路径'</span>)</span><br><span class="line">    xlSht = xlBook.Worksheets(<span class="string">'Sheet1'</span>)<span class="comment">#Sheet1指你要处理的xls文件中的第几页</span></span><br><span class="line">    row = <span class="number">8</span></span><br><span class="line">    col = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= row:<span class="comment">#我这里选择从行开始读：1-1、1-2、1-3...excle是（1，1）开始</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, col+<span class="number">1</span>):</span><br><span class="line">            temp = str(xlSht.Cells(i, j).Value)<span class="comment">#把读入的数据转成字符串；</span></span><br><span class="line">            <span class="keyword">if</span> temp.find(<span class="string">'删去此行'</span>) &gt; <span class="number">-1</span>:<span class="comment">#“删去此行”就是比较字符串内容</span></span><br><span class="line">                xlSht.Rows(i).Delete()<span class="comment">#执行删除第i行；</span></span><br><span class="line"><span class="comment">#xlSht.Cols(j).Delete()#执行列删除</span></span><br><span class="line">                i = i<span class="number">-1</span><span class="comment">#与i=i+1对应，复位操作，因为删掉了此行，原本下一行第i+1行变成了第i行</span></span><br><span class="line">                row = row<span class="number">-1</span><span class="comment">#总行数-1</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        i = i+<span class="number">1</span><span class="comment">#复位操作</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    xlBook.Close(SaveChanges=<span class="number">1</span>)<span class="comment">#关闭excle</span></span><br><span class="line">    <span class="keyword">del</span> xlApp</span><br><span class="line">    print(<span class="string">"删除完成"</span>)</span><br></pre></td></tr></table></figure><p><strong>执行结果：</strong><br><br></p><table><thead><tr><th align="center">第一列</th><th align="center">第二列</th><th align="center">第三列</th></tr></thead><tbody><tr><td align="center">4</td><td align="center">2</td><td align="center">3</td></tr><tr><td align="center">1</td><td align="center">2</td><td align="center">1</td></tr><tr><td align="center">1</td><td align="center">3</td><td align="center">2</td></tr></tbody></table><p>合在一起的话，加个main()函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    Duplicate()</span><br><span class="line">    delete_row()</span><br></pre></td></tr></table></figure><hr><h4>!!!注意坑：</h4><p>假如你是python2.7的话，且Excle表中有汉字的话，在<code>data.drop_duplicates([&#39;第三列&#39;], keep=&#39;first&#39;, inplace=True)</code>和<code>temp = str(xlSht.Cells(i, j).Value)</code>会报类似的错：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR (UnicodeEncodeError): <span class="string">'ascii'</span> codec can<span class="string">'t encode character u'</span>\uff08<span class="string">' in position 9: ordinal not in range(128)</span></span><br></pre></td></tr></table></figure><p>这是因为python2.7没办法处理非ascii编码的，此时需要自己设置将python的默认编码，一般设置为utf8的编码格式。</p><ul><li>解决办法：</li></ul><p>在你python2.7的lib\site-packages文件夹下新建一个sitecustomize.py<br><strong>添加如下内容，设置编码为utf-8</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding=utf8 </span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">reload(sys) </span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)</span><br></pre></td></tr></table></figure><p>保存退出后，记得重启编辑器，再试试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2&gt;Python实现Excel表的去重、删除特定行\列&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;最近分析导出数据的文件为excel表，就想用python来实现一下自动化操作，当然现在还是定制的Demo,还需要改进。&lt;/em&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>【转】Django JSONField SQL注入漏洞（CVE-2019-14234）</title>
    <link href="http://yoursite.com/2019/08/05/CVE-2019-14234/"/>
    <id>http://yoursite.com/2019/08/05/CVE-2019-14234/</id>
    <published>2019-08-05T14:02:17.000Z</published>
    <updated>2019-08-06T13:40:21.168Z</updated>
    
    <content type="html"><![CDATA[<h2>【转】Django JSONField SQL注入漏洞（CVE-2019-14234）</h2><p><em>这两天刚刚爆出来的一个SQL注入漏洞，正好我也在学Django,主要还是参考公司的文档和 P师傅 的博客来学习一下，研究院大佬牛批！P师傅牛批！</em></p><p><strong>关键字：<code>QuerySet.filter()</code>,<code>JSONField</code>,<code>HStoreField</code></strong></p><p><strong>文章转载自</strong>：<a href="https://www.leavesongs.com/PENETRATION/django-jsonfield-cve-2019-14234.html" title="Django JSONField SQL注入漏洞（CVE-2019-14234）分析与影响" target="_blank" rel="noopener">Django JSONField SQL注入漏洞（CVE-2019-14234）分析与影响</a></p><p>作为铁杆Django用户，发现昨天Django进行了更新，且修复了一个SQL注入漏洞。在我印象里这应该是Django第一个SQL注入漏洞，且的确是可能在业务里出现的漏洞，于是进行了分析。</p><p><strong>#0x01 什么是JSONField</strong><br>Django是一个大而全的Web框架，其支持很多数据库引擎，包括Postgresql、Mysql、Oracle、Sqlite3等，但与Django天生为一对儿的数据库莫过于Postgresql了，Django官方也建议配合Postgresql一起使用。</p><p>相比于Mysql，Postgresql支持的数据类型更加丰富，其对JSON格式数据的支持也让这个关系型数据库拥有了NoSQL的一些特点。在Django中也支持了Postgresql的数据类型：</p><ul><li>JSONField</li><li>ArrayField</li><li>HStoreField</li></ul><p>这三种数据类型因为都是非标量，且都能用JSON来表示，我下文就用JSONField统称了。</p><p>我们可以很简单地在Django的model中定义JSONField：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> django.db <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> django.contrib.postgres.fields <span class="keyword">import</span> JSONField</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Collection</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    name = models.CharField(max_length=<span class="number">128</span>, default=<span class="string">'default name'</span>)</span><br><span class="line">    detail = JSONField()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br></pre></td></tr></table></figure><p>然后，我们在视图中，就可以对detail字段里的信息进行查询了。</p><p>比如，detail中存储了一些文章信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>: <span class="string">"Article Title"</span>,</span><br><span class="line">  <span class="string">"author"</span>: <span class="string">"phith0n"</span>,</span><br><span class="line">  <span class="string">"tags"</span>: [<span class="string">"python"</span>, <span class="string">"django"</span>],</span><br><span class="line">  <span class="string">"content"</span>: <span class="string">"..."</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我要查询作者是<code>phit0n</code>的所有文章，就可以使用Django的queryset：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Collection.objects.filter(detail__author=<span class="string">'phith0n'</span>).all()</span><br></pre></td></tr></table></figure><p>非常简单，和我们正常的queryset完全一样，只不过这里的detail是一个JSONField，而下划线后的内容代表着JSON中的键名，而不再是常规queryset时表示的“外键”。</p><p>同理，如果我想查询所有含有python这个<code>tag</code>的文章，可以这样编写queryset：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Collection.objects.filter(detail__tags__contains=<span class="string">'django'</span>).all()</span><br></pre></td></tr></table></figure><p>JSONField的强大让我们能灵活地在关系型数据库与非关系型数据库间轻松地切换，因此在我们的很多业务中都会使用到这个功能。</p><p><strong>#0x02 SQL注入漏洞何来</strong><br>那么，是什么问题导致了这个漏洞？</p><p>我们直接看到JSONField的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JSONField</span><span class="params">(CheckFieldDefaultMixin, Field)</span>:</span></span><br><span class="line">    empty_strings_allowed = <span class="literal">False</span></span><br><span class="line">    description = _(<span class="string">'A JSON object'</span>)</span><br><span class="line">    default_error_messages = &#123;</span><br><span class="line">        <span class="string">'invalid'</span>: _(<span class="string">"Value must be valid JSON."</span>),</span><br><span class="line">    &#125;</span><br><span class="line">    _default_hint = (<span class="string">'dict'</span>, <span class="string">'&#123;&#125;'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_transform</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        transform = super().get_transform(name)</span><br><span class="line">        <span class="keyword">if</span> transform:</span><br><span class="line">            <span class="keyword">return</span> transform</span><br><span class="line">        <span class="keyword">return</span> KeyTransformFactory(name)</span><br></pre></td></tr></table></figure><p>JSONField继承自Field，其实Django中所有字段都继承自Field，其中定义了<code>get_transform</code>函数。</p><p>编写过自定义Field的同学应该知道，Django中有以下两个概念：</p><ol><li>Lookup</li><li>Transform<br>我们以上面给出过的一个例子来说明这两者的区别：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.filter(detail__tags__contains=<span class="string">'django'</span>)</span><br></pre></td></tr></table></figure><p>这个queryset中，<code>__tags</code>是transform，而<code>__contains</code>是lookup。</p><p>他们的区别是：<code>transform</code>表示“如何去找关联的字段”，<code>lookup</code>表示“这个字段如何与后面的值进行比对”。</p><p>正常情况下，<code>transform</code>一般用来在通过外键连接两个表，比如<code>.filter(author__username=&#39;phith0n&#39;)</code>可以表示在author外键连接的用户表中，找到username字段；<code>lookup</code>很多时候是被省略的，比如<code>.filter(username=&#39;phith0n&#39;)</code>表示找到用户名为phith0n的用户，这个被省略的lookup其实就是<code>__exact</code>。</p><p>用伪SQL语句表示就是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WHERE `users`[<span class="number">1</span>] [<span class="number">2</span>] <span class="string">'value'</span></span><br></pre></td></tr></table></figure><p>位置[1]是<code>transform</code>，位置[2]是<code>lookup</code>，比如transform是寻找外键表的字段username，lookup是exact（也就是等于），那么生成的SQL语句就是<code>WHERE users.username = &#39;value&#39;</code>。</p><p>那么，在JSONField中，<code>lookup</code>实际上是没有变的，但是<code>transform</code>从“在外键表中查找”，变成了“在JSON对象中查找”，所以自然需要重写<code>get_transform</code>函数。</p><p><code>get_transform</code>函数应该返回一个可执行对象，你可以理解为工厂函数，执行这个工厂函数，获得一个transform对象。</p><p>而JSONField用的工厂函数是<code>KeyTransformFactory</code>类，其返回的是<code>KeyTransform</code>对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeyTransformFactory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, key_name)</span>:</span></span><br><span class="line">        self.key_name = key_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> KeyTransform(self.key_name, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KeyTransform</span><span class="params">(Transform)</span>:</span></span><br><span class="line">    operator = <span class="string">'-&gt;'</span></span><br><span class="line">    nested_operator = <span class="string">'#&gt;'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, key_name, *args, **kwargs)</span>:</span></span><br><span class="line">        super().__init__(*args, **kwargs)</span><br><span class="line">        self.key_name = key_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">as_sql</span><span class="params">(self, compiler, connection)</span>:</span></span><br><span class="line">        key_transforms = [self.key_name]</span><br><span class="line">        previous = self.lhs</span><br><span class="line">        <span class="keyword">while</span> isinstance(previous, KeyTransform):</span><br><span class="line">            key_transforms.insert(<span class="number">0</span>, previous.key_name)</span><br><span class="line">            previous = previous.lhs</span><br><span class="line">        lhs, params = compiler.compile(previous)</span><br><span class="line">        <span class="keyword">if</span> len(key_transforms) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"(%s %s %%s)"</span> % (lhs, self.nested_operator), [key_transforms] + params</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            int(self.key_name)</span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            lookup = <span class="string">"'%s'"</span> % self.key_name</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lookup = <span class="string">"%s"</span> % self.key_name</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"(%s %s %s)"</span> % (lhs, self.operator, lookup), params</span><br></pre></td></tr></table></figure><p>Django的model最本质的作用是生成SQL语句，所以transform和lookup都需要实现一个名为as_sql的方法用来生成SQL语句。这里原本生成的语句应该是：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WHERE (field-&gt;'[key_name]') = 'value'</span><br></pre></td></tr></table></figure><p>但这里可见，<code>[key_name]</code>位置的json字段名居然是……字符串拼接！</p><p>这就是本漏洞出现的原因。</p><h4>To Be Continue...漏洞复现</h4>Django包含一个JSONField的测试model环境，我还在尝试搭建中，这次只看到了原理内容，感谢大佬们的分享，具体复现效果在P师傅的博客中:[Django JSONField SQL注入漏洞（CVE-2019-14234）分析与影响](https://www.leavesongs.com/PENETRATION/django-jsonfield-cve-2019-14234.html#0x01-jsonfield "Django JSONField SQL注入漏洞（CVE-2019-14234）分析与影响")]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2&gt;【转】Django JSONField SQL注入漏洞（CVE-2019-14234）&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;这两天刚刚爆出来的一个SQL注入漏洞，正好我也在学Django,主要还是参考公司的文档和 P师傅 的博客来学习一下，研究院大佬牛批！P师傅牛批！&lt;/em&gt;&lt;/
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>scrpy的登录验证码</title>
    <link href="http://yoursite.com/2019/08/03/scrapy%E9%AA%8C%E8%AF%81%E7%A0%81%E7%99%BB%E5%BD%95/"/>
    <id>http://yoursite.com/2019/08/03/scrapy验证码登录/</id>
    <published>2019-08-03T14:02:17.000Z</published>
    <updated>2019-08-05T13:34:25.114Z</updated>
    
    <content type="html"><![CDATA[<h3>scrpy的登录验证码</h3><p>之前，做了scrapy的模仿登录练习：<br> -带cookies的直接登录；<br> -<code>scrapy.FormRequest.from_response()</code>登录：当元素中存在<code>action</code>跳转目标时，可以直接传<code>response</code>，<code>from_response()</code>自动查询表单内容，自己只需要传<code>user</code>和<code>pwd</code>就行了。</p><br><p>之后，又遇见了验证码的问题，让我踩了小坑：</p><p>看看<code>66yzm.py</code>的spider主体代码：（这网站有安全问题奥，顺便找的）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> 66<span class="title">yzmSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'66yzm'</span></span><br><span class="line">    allowed_domains = [<span class="string">'66yzm.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.66yzm.com/index/login/index.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        url = <span class="string">'http://www.66yzm.com/index/login/index.html'</span></span><br><span class="line">        formdata = &#123;</span><br><span class="line">            <span class="string">'username'</span>: <span class="string">'123spider'</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">'123123123'</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        img_code = response.xpath(<span class="string">"//img[@style='float: left; cursor: pointer; margin-top: 10px;']/@src"</span>).get()</span><br><span class="line">        print(img_code)</span><br><span class="line">        __token__ = response.xpath(<span class="string">"//input[@name='__token__']/@value"</span>).get()</span><br><span class="line">        img_code_url = <span class="string">'http://www.66yzm.com'</span> + img_code</span><br><span class="line">        captcha = self.parse_captcha(img_code_url)</span><br><span class="line">        print(captcha)</span><br><span class="line">        formdata[<span class="string">'code'</span>] = captcha</span><br><span class="line">        formdata[<span class="string">'__token__'</span>] = __token__</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url, formdata=formdata, callback=self.parse_after_login)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_captcha</span><span class="params">(self,img_code_url)</span>:</span></span><br><span class="line">        urllib.urlretrieve(img_code_url, <span class="string">'catpcha_img.png'</span>)</span><br><span class="line">        image = Image.open(<span class="string">'catpcha_img.png'</span>)</span><br><span class="line">        image.show()</span><br><span class="line">        captcha = raw_input(<span class="string">'shu ru:'</span>)</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_after_login</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> io.open(<span class="string">'next2.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">            fp.write(response.text)</span><br></pre></td></tr></table></figure><p> <code>image</code>能正常的打印出来，可问题是输入过后，验证码100%报错，什么原因呢？</p><p><strong>才发现真的是傻了</strong>： <code>urllib.urlretrieve(img_code_url, &#39;catpcha_img.png&#39;)</code>再次请求url，肯定是带的新的cookies，和我原登录请求肯定对应不上啊！</p><p>更改后的spider：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">headers = &#123;</span><br><span class="line">            <span class="string">'user-agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> 66<span class="title">yzmSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'66yzm'</span></span><br><span class="line">    allowed_domains = [<span class="string">'66yzm.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.66yzm.com/index/login/index.html'</span>]</span><br><span class="line">    login_url=<span class="string">'http://www.66yzm.com/index/login/index.html'</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 重写start_url 方法，获取验证码图片，传递cookie</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [scrapy.Request(url=<span class="string">'http://www.66yzm.com/captcha'</span>,headers=headers,callback=self.get_code,meta=&#123;<span class="string">'cookiejar'</span>:<span class="number">1</span>&#125;)]</span><br><span class="line"> </span><br><span class="line">    <span class="comment">#保存验证码图片，带cookies</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_code</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> open (<span class="string">'code.png'</span>,<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.body)</span><br><span class="line">        <span class="keyword">return</span> [scrapy.Request(url=login_url,callback=self.get_token,meta=&#123;<span class="string">'cookiejar'</span>:response.meta[<span class="string">'cookiejar'</span>]&#125;)]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 获取 __token__ 参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_token</span><span class="params">(self,response)</span>:</span></span><br><span class="line">        __token__ = response.xpath(<span class="string">"//input[@name='__token__']/@value"</span>).extract_first()</span><br><span class="line">        print(__token__)</span><br><span class="line">        code=raw_input(<span class="string">'请输入验证码：'</span>)</span><br><span class="line">        formdata = &#123;</span><br><span class="line">                </span><br><span class="line">            <span class="string">'username'</span>: <span class="string">'123spider'</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">'123123123'</span>,</span><br><span class="line">                <span class="string">'code'</span>: code,</span><br><span class="line">                <span class="string">'__token__'</span>: __token__</span><br><span class="line">            &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># post 方法请求</span></span><br><span class="line">        <span class="keyword">return</span> scrapy.FormRequest(url=login_url,formdata=formdata,callback=self.login,meta=&#123;<span class="string">'cookiejar'</span>:response.meta[<span class="string">'cookiejar'</span>]&#125;)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 验证登录结果</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self,response)</span>:</span></span><br><span class="line"><span class="keyword">with</span> io.open(<span class="string">'next2.html'</span>, <span class="string">'w'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(response.text)</span><br></pre></td></tr></table></figure><p><strong>果然OK了！</strong><br>使用scrapy框架方便的同时，千万不要忘了一些基础的信息包，头啊，cookies啊… …</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3&gt;scrpy的登录验证码&lt;/h3&gt;

&lt;p&gt;之前，做了scrapy的模仿登录练习：&lt;br&gt; -带cookies的直接登录；&lt;br&gt; -&lt;code&gt;scrapy.FormRequest.from_response()&lt;/code&gt;登录：当元素中存在&lt;code&gt;action&lt;/
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>centos7 升级openssh 的踩坑记录</title>
    <link href="http://yoursite.com/2019/07/30/openssh_updata_error/"/>
    <id>http://yoursite.com/2019/07/30/openssh_updata_error/</id>
    <published>2019-07-30T14:02:17.000Z</published>
    <updated>2019-07-30T15:19:21.482Z</updated>
    
    <content type="html"><![CDATA[<h2>centos7 升级openssh到openssh-8.0p1版本</h2><h4>一、警惕 rm -rf</h4><p>因为项目需求，在远程升级服务器openssh时遇到了一些问题，在把升级所需要的一些依赖包打上去之前，先把<code>openssl</code>的一些东西给删了，导致服务器ssh出现了问题。幸好之前开了<code>telnet</code>服务，还能连上后台。</p><p>可这问题并没有解决，<code>openssl</code>的删除，导致<code>yum</code>也无法使用，而<code>telnet</code>远程上传文件又不想ssh 的<code>scp</code>那么方便。</p><p>所以最后还是要到机房，上U盘，所幸机房离我不远。<br><strong>如果远程操作，对于 删除操作 一定要慎重又慎重，最好cp备份一下。</strong></p><h4>二、fdisk 看盘名称问题</h4><p>没错，这个我都能踩坑：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Disk /dev/sdh: 15.5 GB, 15474836480 bytes</span><br><span class="line">255 heads, 63 sectors/track, 2610 cylinders</span><br><span class="line">Units = cylinders of 16065 * 512 = 8225280 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0xe8517189</span><br><span class="line"> </span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/sdh4               1        2610    20964793+   5  Extended</span><br></pre></td></tr></table></figure><p>上机房插上U盘后，直接<code>fdisk</code>找一下盘的名字，我只读了第一行<code>sdh</code>，看到容量差不多，是我的盘，然后<code>mount</code>挂载就出了问题，一直报：写保护error</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost mnt]# mount /dev/sdh /mnt/data1/</span><br><span class="line">mount: block device /dev/sdh is write-protected, mounting read-only</span><br><span class="line">mount: cannot mount block device /dev/sdh read-only</span><br></pre></td></tr></table></figure><p>这我人傻了，我U盘怎么就写保护了？难道是因为我U盘 FAT32的问题，我又去找了相应的挂载指令：<code>[root@localhost mnt]# mount -t vfat /dev/sdh /mnt/data1</code><br>还是不行，依旧 写保护 报错；</p><p>最后才发现时真的U盘写错了… … 在Device 下面才是U盘的名称<code>sdh4</code>;<br>明明之前重装Centos7 的时候还在edit模式下看到过的<code>device</code>名称；<br><strong>注意 fdisk 的方法吧。</strong></p><h4>三、重要文件备份一定做好</h4><p>依赖包打上去了以后，剩下的就是按步骤安装、写配置、编译就完了。这里有个坑就是<code>/etc/ssh/sshd_config</code>这个文件一定要实现备份好。<br>因为在操作中要首先<strong>删除</strong>这些原有的配置文件（对于删除操作的慎重），而且备份后方便去替代编译make以后那个新生成的sshd_config配置文件，不用很麻烦的去修改内容，各台服务器有各台服务器的环境，真的容易出错；</p><p>之后还有记得删除原 <code>/root/.ssh/ known_hosts</code>的内容；因为升级之后远程连接主机发现密匙发生变化，需要去重新获取。</p><h5>To Be Continue... ...</h5><p>升级完了openssh之后，web链接出现了问题，初步判断时443挂了，问题还在排查… …<br>（PS：不是运维的运维踩坑记录）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2&gt;centos7 升级openssh到openssh-8.0p1版本&lt;/h2&gt;


&lt;h4&gt;
一、警惕 rm -rf
&lt;/h4&gt;

&lt;p&gt;因为项目需求，在远程升级服务器openssh时遇到了一些问题，在把升级所需要的一些依赖包打上去之前，先把&lt;code&gt;openssl&lt;/c
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>Crawl_spider模块</title>
    <link href="http://yoursite.com/2019/07/29/Crawl_spider/"/>
    <id>http://yoursite.com/2019/07/29/Crawl_spider/</id>
    <published>2019-07-29T14:02:17.000Z</published>
    <updated>2019-08-06T13:58:56.274Z</updated>
    
    <content type="html"><![CDATA[<h2>Crawl_spider</h2><p><code>CrawlSpider</code>也继承自<code>Spider</code>，所以具备它的所有特性，这些特性上章已经讲过了，就再在赘述了，这章就讲点它本身所独有的。<br>crawlspider比传统spider更加简便、智能；我们网站的url都是有一定规则的，像django，在<code>view</code>中定义的<code>urls</code>规则就是正则表示的。那么就可以根据这个特性来设计爬虫，而不是每次都要用spider分析页面格式，拆解源码，scrapy提供了CrawlSpider处理此需求。</p><p>看看CrawlSpider类的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    rules = ()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *a, **kw)</span>:</span></span><br><span class="line">        super(CrawlSpider, self).__init__(*a, **kw)</span><br><span class="line">        self._compile_rules()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_request</span><span class="params">(self, rule, link)</span>:</span></span><br><span class="line">        r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">        r.meta.update(rule=rule, link_text=link.text)</span><br><span class="line">        <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        seen = set()</span><br><span class="line">        <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">            links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</span><br><span class="line">                     <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">            <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">                links = rule.process_links(links)</span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">                seen.add(link)</span><br><span class="line">                r = self._build_request(n, link)</span><br><span class="line">                <span class="keyword">yield</span> rule.process_request(r)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        rule = self._rules[response.meta[<span class="string">'rule'</span>]]</span><br><span class="line">        <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> callback:</span><br><span class="line">            cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</span><br><span class="line">            cb_res = self.process_results(response, cb_res)</span><br><span class="line">            <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</span><br><span class="line">                <span class="keyword">yield</span> requests_or_item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</span><br><span class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</span><br><span class="line">                <span class="keyword">yield</span> request_or_item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> callable(method):</span><br><span class="line">                <span class="keyword">return</span> method</span><br><span class="line">            <span class="keyword">elif</span> isinstance(method, six.string_types):</span><br><span class="line">                <span class="keyword">return</span> getattr(self, method, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</span><br><span class="line">        <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</span><br><span class="line">            rule.callback = get_method(rule.callback)</span><br><span class="line">            rule.process_links = get_method(rule.process_links)</span><br><span class="line">            rule.process_request = get_method(rule.process_request)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, *args, **kwargs)</span>:</span></span><br><span class="line">        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        spider._follow_links = crawler.settings.getbool(</span><br><span class="line">            <span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">        super(CrawlSpider, self).set_crawler(crawler)</span><br><span class="line">        self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>其中的方法很多而且很详细，这里先介绍三种主要常用的方法：</strong></p><p><strong>rules：</strong><br>它是一个列表，存储的元素时<code>Rule</code>类的实例，其中每一个实例都定义了一种采集站点的行为。如果有多个<code>rule</code>都匹配同一个链接，那么位置下标最小的一个<code>rule</code>将会被使用。 CrawlSpider本身有去重的行为。</p><p><strong>callback：</strong><br>该值可以是一个方法，也可以是一个字符串（spider实例中一个方法的名称）。它就是一个回调方法，它的作用是在此处调用一个指定的方法。<br>但要注意的是：要慎用<code>parse</code>做为回调方法，因为这边的<code>parse</code>已经不像传统spider类中的那样没有具体操作，调用同名的<code>parse</code>可能会造成冲突。</p><p><strong>follow：</strong><br>是一个布尔对象，表示是当前<code>response</code>否继续采集。如果<code>callback</code>是None，那么它就默认为True，否则为False。</p><hr><h4>整个数据的流向如下图所示：</h4><p><img src="https://1j0ker.github.io/md_img/CrwalSpider.jpg" alt="exmple1.md"></p><hr><h4>CrawlSpider爬取微信小程序文章</h4><p><strong>spider.py:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> WXapp.items <span class="keyword">import</span> WxappItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WxappSpiderSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'wxapp_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'wxapp-union.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.wxapp-union.com/portal.php?mod=list&amp;catid=2&amp;page=1'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r".+mod=list&amp;catid=2&amp;page=\d"</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r".+article.+\.html"</span>), callback=<span class="string">"parse_item"</span>, follow=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.xpath(<span class="string">"//h1[@class='ph']/text()"</span>).get()</span><br><span class="line">        author = response.xpath(<span class="string">"//p[@class='authors']//a/text()"</span>).get()</span><br><span class="line">        content_json = response.xpath(<span class="string">"//td[@id='article_content']//text()"</span>).getall()<span class="comment">#很多标签有内容，使用getall()全部获得，list型数据</span></span><br><span class="line">        content = <span class="string">""</span>.join(content_json).strip()<span class="comment">#转成字符串形式</span></span><br><span class="line">        item = WxappItem(title=title, author=author, content=content)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><ul><li><code>CrawlSpider</code>使用的重点就是<code>rules</code>规则的构造，确保构造出所允许的采集站点；</li><li>item.py中更改相应的<code>WxappItem</code>方法；</li></ul><br><br>(PS:T-ara十周年快乐啊!)]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2&gt;Crawl_spider&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;CrawlSpider&lt;/code&gt;也继承自&lt;code&gt;Spider&lt;/code&gt;，所以具备它的所有特性，这些特性上章已经讲过了，就再在赘述了，这章就讲点它本身所独有的。&lt;br&gt;crawlspider比传统spide
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架初学习</title>
    <link href="http://yoursite.com/2019/07/22/scrapy%E5%88%9D%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/07/22/scrapy初学习/</id>
    <published>2019-07-22T14:02:17.000Z</published>
    <updated>2019-07-24T13:38:14.296Z</updated>
    
    <content type="html"><![CDATA[<h3>Scrapy框架</h3>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下：<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_1.jpg" alt="scrapy框架_1"></p><h3> 实例</h3><p><strong>利用scrapy框架爬取糗事百科内容</strong>：</p><ul><li>qsbk.py(spider.py)：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.selector.unified <span class="keyword">import</span> SelectorList</span><br><span class="line"><span class="keyword">from</span> test_1.items <span class="keyword">import</span> Test1Item<span class="comment">#把在items里定义的传输数据类型导入</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.qiushibaike.com/text/page/1/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        duanzidivs = response.xpath(<span class="string">"//div[@id='content-left']/div"</span>)</span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="comment"># print(type(duanzidivs))#返回selectorList类型，可遍历</span></span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="keyword">for</span> duanzidiv <span class="keyword">in</span> duanzidivs:</span><br><span class="line">            <span class="comment">#duanzidiv为selector类型，依旧可以使用xpath</span></span><br><span class="line">            author = duanzidiv.xpath(<span class="string">".//h2/text()"</span>).get().strip()<span class="comment">#.//在当前目录下寻找h2;get()将selector型数据转换成为unicode;strip()去除空格；</span></span><br><span class="line">            content = duanzidiv.xpath(<span class="string">".//div[@class='content']//text()"</span>).getall()<span class="comment">#getall()返回list型数据；</span></span><br><span class="line">            content = <span class="string">""</span>.join(content).strip()<span class="comment">#join()将List型数据转化为字符串</span></span><br><span class="line">            <span class="comment">#print(author, '\n')</span></span><br><span class="line">            <span class="comment">#print(content,'\n')</span></span><br><span class="line">            <span class="comment">#duanzi = &#123;"author":author, "content":content&#125;#创建一个字典</span></span><br><span class="line">            <span class="comment">#yield = duanzi</span></span><br><span class="line">            item = Test1Item(author=author, content=content)<span class="comment">#直接调用定义好的类型</span></span><br><span class="line">            <span class="keyword">yield</span> item<span class="comment">#yield 把普通函数变成生成器；</span></span><br></pre></td></tr></table></figure><p>这里的重点时<code>yield</code>，具体包含了scrapy爬虫的迭代运行 ：<a href="https://blog.csdn.net/qq_40695895/article/details/82882761" title="scrapy中 yield使用详解" target="_blank" rel="noopener">scrapy中 yield使用详解</a>；</p><br><ul><li>利用scrapy的items的实例来传递参数，items.py：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Item</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    author = scrapy.Field()<span class="comment">#将需要传递的数据定义成类型；在spider里导入数据类型；</span></span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure><p> 以及3种存储方式，比较优化，pipelines.py：</p><p> 1.利用json模块，转换item；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> io<span class="comment">#open莫名报错，使用io模块open</span></span><br><span class="line"><span class="comment">#1.利用json模块，转换item；</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">         self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)<span class="comment">#创建一个duanzi.json文件，写方式，可以在初始化函数中，也可以在open_spider()中</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">         print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = open("duanzi.json",'w',encoding='utf-8')</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">         item_json = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>)<span class="comment">#注意item要先转成程字典形式；把spider中传过来的duanzi转化成为json类型;ensure_ascii=False保存成中文</span></span><br><span class="line">         self.fp.write(item_json+<span class="string">'\n'</span>)<span class="comment">#将duanzi写入文件</span></span><br><span class="line">         <span class="keyword">return</span> item</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">         self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">         print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><p>2.使用exporters.JsonItemExporter模块，bytes写入;结果一个列表内放所有字典：先将item数据放入内存中，但item过大时，消耗内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"><span class="comment">#2.使用exporters.JsonItemExporter模块，bytes写入;结果一个列表内放所有字典：先将item数据放入内存中，item过大时，消耗内存</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'wb'</span>)<span class="comment">#创建一个duanzi.json文件，二进制写方式，二进制方法就没有编码了</span></span><br><span class="line">        self.exporter = JsonItemExporter(self.fp,ensure_ascii=<span class="literal">False</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = io.open('duanzi.json','wb')</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">        print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><p>3.使用exporters.JsonLinesItemExporter模块，bytes写入;结果一个字典一行,清晰安全不占用内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonLinesItemExporter</span><br><span class="line"><span class="comment">#3.使用exporters.JsonLinesItemExporter模块，bytes写入;结果一个字典一行</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'wb'</span>)<span class="comment">#创建一个duanzi.json文件，二进制写方式，二进制方法就没有编码了</span></span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp,ensure_ascii=<span class="literal">False</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = io.open('duanzi.json','wb')</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">        print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><ul><li>settings.py的参数不要忘记设置：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span><span class="comment">#关闭查找robottxt页面</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">   <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'</span></span><br><span class="line">&#125;<span class="comment">#设置常用头信息</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'test_1.pipelines.Test1Pipeline'</span>: <span class="number">300</span>,<span class="comment">#值越小越先执行</span></span><br><span class="line">&#125;<span class="comment">#开启pipelines</span></span><br></pre></td></tr></table></figure><p><strong>效果：</strong><br>爬取的数据被存放在duanzi.json中：<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_2.jpg" alt="scrapy框架_2"></p><h4>多页面爬取</h4><p>还是寻找页码所在的特征：</p><ul><li>在当前<code>&lt;div&gt;</code>标签下的最后一个<code>&lt;li&gt;</code>里存放着下一页的url;</li><li>在最后一页，也就是13页下，<code>&lt;li&gt;</code>标签里没有存放下一页的链接；<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_3.jpg" alt="scrapy框架_3"></li></ul><p><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_4.jpg" alt="scrapy框架_4"></p><p>所以可以通过判断是否能获取到<code>next_url</code>来控制爬取的页数：<br><strong>qsbk.py(spider):</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.selector.unified <span class="keyword">import</span> SelectorList</span><br><span class="line"><span class="keyword">from</span> test_1.items <span class="keyword">import</span> Test1Item<span class="comment">#把在items里定义的传输数据类型导入</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.qiushibaike.com/text/page/1/'</span>]</span><br><span class="line">    base_domain = <span class="string">'https://www.qiushibaike.com'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        duanzidivs = response.xpath(<span class="string">"//div[@id='content-left']/div"</span>)</span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="comment"># print(type(duanzidivs))#返回selectorList类型，可遍历</span></span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="keyword">for</span> duanzidiv <span class="keyword">in</span> duanzidivs:</span><br><span class="line">            <span class="comment">#duanzidiv为selector类型，依旧可以使用xpath</span></span><br><span class="line">            author = duanzidiv.xpath(<span class="string">".//h2/text()"</span>).get().strip()<span class="comment">#.//在当前目录下寻找h2;get()将selector型数据转换成为unicode;strip()去除空格；</span></span><br><span class="line">            content = duanzidiv.xpath(<span class="string">".//div[@class='content']//text()"</span>).getall()<span class="comment">#getall()返回list型数据；</span></span><br><span class="line">            content = <span class="string">""</span>.join(content).strip()<span class="comment">#join()将List型数据转化为字符串</span></span><br><span class="line">            <span class="comment">#print(author, '\n')</span></span><br><span class="line">            <span class="comment">#print(content,'\n')</span></span><br><span class="line">            <span class="comment">#duanzi = &#123;"author":author, "content":content&#125;#创建一个字典</span></span><br><span class="line">            <span class="comment">#yield = duanzi</span></span><br><span class="line">            item = Test1Item(author=author, content=content)<span class="comment">#直接调用定义好的类型</span></span><br><span class="line">            <span class="keyword">yield</span> item<span class="comment">#yield 把普通函数变成生成器；</span></span><br><span class="line">        next_url = response.xpath(<span class="string">"//ul[@class='pagination']/li[last()]/a/@href"</span>).get()<span class="comment">#取最后一个&lt;li&gt;标签，如果里面没有&lt;a href=''&gt;标签内容了，最说明是最后一页了</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> next_url:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            page_num = self.base_domain+next_url</span><br><span class="line">            print(page_num)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(page_num,callback=self.parse)<span class="comment">#利用Request中的callback再次调用解析parse(),注意链接双方同类型</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3&gt;Scrapy框架&lt;/h3&gt;
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。
其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Ama
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>CSP的防护与绕过</title>
    <link href="http://yoursite.com/2019/07/20/CSP%E7%9A%84%E9%98%B2%E6%8A%A4%E4%B8%8E%E7%BB%95%E8%BF%87/"/>
    <id>http://yoursite.com/2019/07/20/CSP的防护与绕过/</id>
    <published>2019-07-20T14:02:17.000Z</published>
    <updated>2019-07-22T14:41:53.587Z</updated>
    
    <content type="html"><![CDATA[<h1>CSP的概念</h1><h3> CSP的前端防护</h3>XSS向来是安全方面所重视的一大难题，虽然近年来在OWASP中下降了一些，但仍是实际生产环境中不可忽略的问题。对于一个基本的XSS漏洞页面，它发生的原因往往是从用户输入的数据到输出没有有效的过滤，就比如下面的这个范例代码。<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">$a = $_GET[<span class="string">'a'</span>];</span><br><span class="line"><span class="keyword">echo</span> $a;</span><br></pre></td></tr></table></figure><p>毫无过滤，我们可以很轻松的构造一种payload来实现XSS：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=&lt;script&gt;alert(1)&lt;/script&gt;</span><br><span class="line">a=&lt;img/src=1/onerror=alert(1)&gt;</span><br><span class="line">a=&lt;svg/onload=alert(1)&gt;</span><br></pre></td></tr></table></figure><p>所以大多数会使用<code>htmlspecialchars</code>函数来过滤输入，或者更严格一些，使用<code>htmlspecialchars</code> + 黑名单的形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&amp; (AND) =&gt; &amp;amp;</span><br><span class="line">" (双引号) =&gt; &amp;quot; (当ENT_NOQUOTES没有设置的时候) </span><br><span class="line">' (单引号) =&gt; &amp;#039; (当ENT_QUOTES设置) </span><br><span class="line"><span class="tag">&lt; (小于号) =&gt;</span> &amp;lt; </span><br><span class="line">&gt; (大于号) =&gt; &amp;gt; </span><br><span class="line">...</span><br><span class="line">on\w+=</span><br><span class="line">script</span><br><span class="line">svg</span><br><span class="line">iframe</span><br><span class="line">link</span><br><span class="line">...</span><br><span class="line">% * + , – / ; <span class="tag">&lt; = &gt;</span> ^ | `</span><br></pre></td></tr></table></figure><p>但是实际上的生产环境永远要比以前的多，XSS的插入点，可能是标签的属性，标签的值，标签的头部等等…</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#123;插入点&#125;</span>&gt;</span>(没有引号)</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span>&#123;插入点&#125;<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>见过的那么多XSS漏洞，大多数漏洞的产生点，都是过滤函数忽略的地方。</strong></p><h4>所以CSP诞生了：</h4>**CSP**是网页安全政策(Content Security Policy)的缩写。是一种由开发者定义的安全性政策申明，通过CSP所约束的责任指定可信的内容来源，（内容可以是指脚本、图片、style 等远程资源）。通过CSP协定，可以防止XSS攻击，让web处一个安全运行的环境中。 CSP 的实质就是白名单制度，开发者明确告诉客户端，哪些外部资源可以加载和执行，等同于提供白名单。它的实现和执行全部由浏览器完成，开发者只需提供配置。CSP 大大增强了网页的安全性。攻击者即使发现了漏洞，也没法注入脚本，除非还控制了一台列入了白名单的可信主机。<br>**特点**就是他是在浏览器层面做的防护，是和同源策略同一级别，除非浏览器本身出现漏洞，否则不可能从机制上绕过。<p>CSP只允许被认可的JS块、JS文件、CSS等解析，只允许向指定的域发起请求。<br>一个简单的 HTTP 头信息CSP规则:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: script-src 'self'; object-src 'none'; style-src cdn.example.org third-party.org; child-src https:");</span><br></pre></td></tr></table></figure><p>其中的规则指令分很多种，每种指令都分管浏览器中请求的一部分，上面代码中，CSP 做了如下配置:</p><ul><li><p>脚本：只信任当前域名</p></li><li><p>object标签：不信任任何URL，即不加载任何资源</p></li><li><p>样式表：只信任cdn.example.org和third-party.org</p></li><li><p>框架（frame）：必须使用HTTPS协议加载</p></li><li><p>其他资源：没有限制</p></li></ul><p>启用后，不符合 CSP 的外部资源就会被阻止加载。<br><strong>详细的指令配置参考</strong>：<a href="https://content-security-policy.com" title="https://content-security-policy.com" target="_blank" rel="noopener">https://content-security-policy.com</a></p><p>简单来说，针对不同来源，不同方式的资源加载，都有相应的加载策略。</p><p>我们可以说，如果一个站点有足够严格的CSP规则，那么XSS or CSRF就可以从根源上被防止。</p><p>但事实真的是这样吗？</p><h3>CSP Bypass</h3>- 最普通最常见的CSP规则，只允许加载当前域的js:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: default-src 'self'; script-src 'self' ");</span><br></pre></td></tr></table></figure><p><strong>但是！</strong>如果站内有上传图片的地方，如果我们上传一个内容为js的图片，图片就在网站的当前域下了：<br>那么直接加载图片：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'upload/test.js'</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><br>- 当你发现设置self并不安全的时候，可能会选择把静态文件的可信域限制到目录，看上去好像没什么问题了：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header(" Content-Security-Policy: default-src 'self '; script-src http://127.0.0.1/static/ ");</span><br></pre></td></tr></table></figure><p>但是如果可信域内存在一个可控的重定向文件，那么CSP的目录限制就可以被绕过。</p><p>假设static目录下存在一个302文件：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Static/302.php</span><br><span class="line"></span><br><span class="line">&lt;?php Header(&quot;location: &quot;.$_GET[&apos;url&apos;])?&gt;</span><br></pre></td></tr></table></figure><p>首先上传一个内容为JS的上传一个test.jpg；<br>然后通过302.php跳转到upload目录加载js就可以成功执行：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"static/302.php?url=upload/test.js"</span>&gt;</span></span><br></pre></td></tr></table></figure><br>- CSP除了阻止不可信js的解析以外，还有一个功能是组织向不可信域的请求:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: default-src 'self'; script-src 'self' ");</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://lorexxar.cn/1.jpg"</span>&gt;</span> #尝试加载域外图片，被阻止</span><br></pre></td></tr></table></figure><p>但是在CSP1.0中，对于link的限制并不完整，不同浏览器包括chrome和firefox对CSP的支持都不完整，每个浏览器都维护一份包括CSP1.0、部分CSP2.0、少部分CSP3.0的CSP规则：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"prefetch"</span> <span class="attr">href</span>=<span class="string">"http://lorexxar.cn"</span>&gt;</span> #(H5预加载)(only chrome)</span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"dns-prefetch"</span> <span class="attr">href</span>=<span class="string">"http://lorexxar.cn"</span>&gt;</span> #（DNS预加载）</span><br></pre></td></tr></table></figure><p>都会造成域外加载；<br><br></p><p>还有很多类似jsonp和CSP相冲突的组件，造成了CSP无法完全的限制安全域，或者说控制安全域内的资源真正来源是否安全… …<br><br><br>当然随着CSP的升级，一些域控问题得到改善，但有些问题还是无法解决：<br>最要命的，比如：<strong>dom xss</strong>;</p><h5>DOM XSS</h5>现代浏览器大多都有缓存机制，但页面中没有修改或者不需要再次请求后台的时候，浏览器就会从缓存中读取页面内容，`location.hash`就是一个典型的例子；<p>如果JS中存在操作location.hash导致的xss，那么这样的攻击请求不会经过后台；<br>或者通过通过CSS选择器来读取页面内容，页面只变化了CSS，构造纯静态的XSS，不过CSP筛选：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*<span class="selector-attr">[attribute^="a"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=a"</span>)&#125; </span><br><span class="line">*<span class="selector-attr">[attribute^="b"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=b"</span>)&#125; </span><br><span class="line">*<span class="selector-attr">[attribute^="c"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=c"</span>)&#125; <span class="selector-attr">[...]</span></span><br></pre></td></tr></table></figure><p>当匹配到对应的属性，页面就会发出相应的请求。<br>上面是常见的3种绕过；<br><a href="https://paper.seebug.org/423/#0x03-csp-bypass" title="更多绕过的方法。" target="_blank" rel="noopener">更多绕过方法。</a></p><h3>总结</h3>CSP虽然有着严格的白名单控制，但仍然只能减轻以偷取信息为目的的XSS行为，对于纯粹的破坏行为则无能为力，特别是DOM Based XSS，所以对于防范XSS还是需要数据的输入校验以及数据输出的Encode，前端浏览器与后端Nginx，Apache的共同协作；<h5>ref:</h5>[1] [https://paper.seebug.org/91/](https://paper.seebug.org/91/ "https://paper.seebug.org/91")<p>[2] <a href="https://lorexxar.cn" title="https://lorexxar.cn" target="_blank" rel="noopener">https://lorexxar.cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;CSP的概念&lt;/h1&gt;


&lt;h3&gt; CSP的前端防护&lt;/h3&gt;
XSS向来是安全方面所重视的一大难题，虽然近年来在OWASP中下降了一些，但仍是实际生产环境中不可忽略的问题。
对于一个基本的XSS漏洞页面，它发生的原因往往是从用户输入的数据到输出没有有效的过滤，就比如下
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>寻找 X-Forwarded-For 的真实IP的一些理解(续)</title>
    <link href="http://yoursite.com/2019/07/17/XFFsearch_real_ip/"/>
    <id>http://yoursite.com/2019/07/17/XFFsearch_real_ip/</id>
    <published>2019-07-17T14:02:17.000Z</published>
    <updated>2019-07-18T15:01:27.565Z</updated>
    
    <content type="html"><![CDATA[<br><h1>寻找 X-Forwarded-For 的真实IP</h1><br>*前言：为什么要寻找真实IP？*因为在实际项目中，用户ip的获取很重要。通过报障用户的ip来快速定位用户的请求日志，在威胁分析中定位攻击源目IP。<br>紧接上次的X-Forward-For的介绍，我们能了解到，XFF头中可能会存在真实IP，不真实的原因总结为两点：<ul><li>人为修改HTTP包头的X-Forward-For,导致真实IP的混杂；</li><li>用户的正向代理访问：这个暂时没有解决办法，而且也是我在做威胁分析时遇到的异常头疼的问题之一；</li></ul><p>所以，接下来，我所讨论的就是，当人为去修改XFF头时，我们能有什么办法去寻找到真实IP，或者说去防止这种情况的发生。</p><hr><br><h3>如何获得真实IP？</h3><br><ul><li><h4>使用 X-Forwarded-For + Real IP 模块:</h4>可以使用nginx的 `ngx_http_realip_module` 模块，从 X-Forwarded-For 或其他属性中提取真实IP。此处以 X-Forwarded-For 结合该模块为例子，需要做两件事：</li><li>请求途径的各代理需要设置 <code>proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for</code>;</li><li>利用 realip 模块获取真实IP<br>这里proxy3的部分配置(proxy3将请求直接转发到后端服务)，如下：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  ...</span><br><span class="line">    location / &#123;</span><br><span class="line">        set_real_ip_from ip1(proxy1的IP); </span><br><span class="line">        real_ip_header    X-Forwarded-For;</span><br><span class="line">        real_ip_recursive on;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>set_real_ip_from</code>: 表示从何处获取真实IP(解决安全问题，只认可自己信赖的IP)，可以是IP或子网等， 可以设置多个set_real_ip_from，上面的例子中就设置的时Proxy1的IP；</li><li><code>real_ip_header</code>：表示从HTTP包头的哪个header属性中获取真实IP；</li><li><code>real_ip_recursive</code>：递归检索真实IP，若从 X-Forwarded-For 中获取，则需递归检索；若像从X-Real-IP中获取，则无需递归。</li></ul><p>基于上一章日志测试格式的测试数据，试验结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1(IP1) 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1, 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>这里解释一下[Proxy3]中的第一个IP和最后一个IP为什么是我们的真实IP了：</p><ul><li>我们在Proxy3里的<code>set_real_ip_from</code>设置了真实IP的获取来源：Proxy1，而Proxy1 <code>$remote_addr</code>(再次强调，这个不可被更改)的就是我们的真实IP；</li><li>而且在Proxy1中根据XFF的规则，此时用户的代理IP就是他本身的IP；</li></ul><p>最终，proxy3 的 <code>$remote_addr</code> 也拿到了客户端的真实IP 123.123.123.123，然后 proxy3 将 remote_addr 传递到后端服务中去；<br><br></p><ul><li><h4>使用X-Forwarded-For + 安全设置:</h4>由于客户端可以自行传递X-Forwarded-For，因此，可以在第一个代理处重置其值，达到忽略客户端传递的X-Forwarded-For的效果，因为proxy1的 `$remote_addr` 是客户端真实IP:</li></ul><p>在 proxy1 中进行如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_set_header X-Forwarded-For $remote_addr;</span><br></pre></td></tr></table></figure><p>与此类似的是设置Proxy1中 X-Real-IP:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_set_header X-Real-IP $remote_addr;</span><br></pre></td></tr></table></figure><p>配置下测试日志格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_format proxy1 '"[proxy1]" $http_x_real_ip "$request" $status';</span><br><span class="line">log_format proxy2 '"[proxy2]" $http_x_real_ip "$request" $status';</span><br><span class="line">log_format proxy3 '"[proxy3]" $http_x_real_ip "$request" $status';</span><br></pre></td></tr></table></figure><p>看一下打印结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" - "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>在最后的Proxy3中也打印出了真实IP：123.123.123.123</p><hr><br><h2>网站使用CDN之后如何禁止恶意用户真实IP</h2><br>这是前几天做威胁分析时遇到的麻烦问题；在威胁分析中，各种层出不断的恶意扫描、拉取、注入等图谋不轨行为令人头大，当然我们一般可以直接通过 iptables 、 Nginx 的 deny 指令或者是程序来 ban 掉这些恶意请求IP；但是对于套了一层 CDN 或代理的网站，这些方法可能就失效了。尤其是个人网站，可能就一台 VPS，然后套一个免费的 CDN 就行走在互联网了。并不是每个 CDN 都能精准的拦截各种恶意请求的，更闹心的是很多 CDN 还不支持用户在 CDN 上添加 BAN 规则...<p>基于上篇文章的内容的基础上，当网站部署了CDN后，用户又是何种访问模式呐？</p><ul><li>用户访问使用了 CDN 的网站<br>浏览器 –&gt; DNS 解析 –&gt; CDN 节点 –&gt; WEB 数据处理 –&gt; 数据吐到浏览器渲染展示</li><li>用户通过代理上网访问了我们的网站<br>浏览器 –&gt; 代理上网 –&gt; DNS 解析 –&gt; CDN 节点 –&gt; WEB 数据处理 –&gt; 数据吐到浏览器渲染展示</li></ul><p>对于这两种访问模式，直接的<code>iptables -I INPUT -s 用户的ip -j DROP</code> 或者 <code>deny</code>都失去了作用，因为 iptables 和 deny 都只能针对直连 IP，而这2种模式中，WEB 服务器直连 IP 是 CDN 节点或者代理服务器，此时使用 iptable 或 deny 就只能把 CDN 节点 或代理 IP 给封了，可能误杀一大片正常用户了，而真正的罪魁祸首轻轻松松换一个代理 IP 又能继续请求了。</p><p><strong>所以如何获取真实IP非常关键</strong><br>其实也就是利用Nginx上的http模块操作 <code>$http_x_forwarded_for</code>和<code>$clientRealIp</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>获取用户真实IP，并赋值给变量$clientRealIP</span><br><span class="line">map $http_x_forwarded_for  $clientRealIp &#123;</span><br><span class="line">        ""      $remote_addr;</span><br><span class="line">        ~^(?P&lt;firstAddr&gt;[0-9\.]+),?.*$  $firstAddr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实也就是上面讲到的，匹配了 <code>$http_x_forwarded_for</code> 的第一个值(安全设置下的X-Forward-For第一个值就为源IP)，而且代码中还配合使用了 <code>$remote_addr</code>，避免了直接访问下时 <code>$http_x_forwarded_for</code>为空；替代了传统的 <code>$remote_addr</code>；</p><h4>其实都是对用户未使用高度匿名代理、普通匿名代理的处理</h4> <p><a href="https://blog.csdn.net/bytxl/article/details/15334153" title="3种正向代理方式介绍" target="_blank" rel="noopener">3种正向代理方式的介绍</a></p><p>既然已经拿到了真实 IP，利用Nginx的判断，<code>return 403</code>就好了；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>如果真实IP为 123.123.123.123，那么返回403</span><br><span class="line">if ($clientRealIp ~* "121.42.0.18|121.42.0.19") &#123;</span><br><span class="line">        #add_header Content-Type text/plain;</span><br><span class="line">        return 403;</span><br><span class="line">        break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到这里我对<code>X-Forward-For</code>的一些理解就介绍完了，后面还有对3种代理方式的学习，以及透明代理，普通匿名代理和高级匿名代理的理解；<br>贴一下参考：<a href="https://blog.51cto.com/z00w00/1031287" title="透明代理、正向代理、反向代理" target="_blank" rel="noopener">透明代理、正向代理、反向代理</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;br&gt;
&lt;h1&gt;寻找 X-Forwarded-For 的真实IP&lt;/h1&gt;
&lt;br&gt;
*前言：为什么要寻找真实IP？*
因为在实际项目中，用户ip的获取很重要。通过报障用户的ip来快速定位用户的请求日志，在威胁分析中定位攻击源目IP。
&lt;br&gt;
紧接上次的X-Forward-F
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>Python多线程的变量影响</title>
    <link href="http://yoursite.com/2019/07/15/py_threading/"/>
    <id>http://yoursite.com/2019/07/15/py_threading/</id>
    <published>2019-07-15T14:02:17.000Z</published>
    <updated>2019-07-16T15:09:18.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python多线程的变量影响"><a href="#Python多线程的变量影响" class="headerlink" title="Python多线程的变量影响"></a>Python多线程的变量影响</h1><h3 id="example1：sleep-控制多线程的变量"><a href="#example1：sleep-控制多线程的变量" class="headerlink" title="example1：sleep()控制多线程的变量"></a>example1：sleep()控制多线程的变量</h3><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">num = <span class="number">100</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> num</span><br><span class="line">    print(<span class="string">"work_1 ---- %d\n"</span> % num)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"work_2 ---- %d\n"</span> % num)</span><br><span class="line">        time.sleep(<span class="number">1</span>)<span class="comment">#sleep()控制了work_2线程顺序</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"num最初：%d\n"</span> %num)</span><br><span class="line">    t_1 = threading.Thread(target=work_1)</span><br><span class="line">    t_2 = threading.Thread(target=work_2)</span><br><span class="line">    t_2.start()</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)<span class="comment">#sleep()控制了线程顺序</span></span><br><span class="line">    t_1.start()<span class="comment">#线程只能start一次</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">"num最后：%d\n"</span> %num)</span><br></pre></td></tr></table></figure><p>通过运行结果，发现<code>work_2()</code>中的sleep(1)使得work_2的线程未执行完函数，num便被<code>work_1()</code>调用，所以work_1的num不是103；<br><br><br><img src="https://1j0ker.github.io/md_img/threading_1.jpg" alt="exmple1.md"><br><br></p><h3 id="example2：多线程各自非顺序计算对全局变量的影响"><a href="#example2：多线程各自非顺序计算对全局变量的影响" class="headerlink" title="example2：多线程各自非顺序计算对全局变量的影响"></a>example2：多线程各自非顺序计算对全局变量的影响</h3><p><br>—————————————————————————-<br><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">g_num = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_threading</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> g_num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        g_num += <span class="number">1</span></span><br><span class="line">     </span><br><span class="line">    print(<span class="string">"work_threading ---- %d\n"</span> % g_num)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> g_num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        g_num += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    print(<span class="string">"work ---- %d\n"</span> % g_num)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">print(<span class="string">"创建多线程之前g_num: %d"</span>%g_num)</span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=work_threading, args=(<span class="number">100000</span>,))</span><br><span class="line">t.start()</span><br><span class="line"><span class="comment">#time.sleep(1)</span></span><br><span class="line">work(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><p><em>看一下执行结果：</em></p><p><img src="https://1j0ker.github.io/md_img/threading_val2.jpg" alt="exmple2.md"><br><br><br>&#8195;&#8195;能看到，每次执行结果都可能都不相同，原因就是，<code>work_threading()</code>线程调用时使其独自计算，代码继续下读，到<code>work()</code>导致在未完成完整的函数计算时，被双方调用的g_num可能是非最终结果（正常计算应该是100000），造成了最后参数的混乱不一；<br><em>（<code>work()</code>也可以是多线程函数，这里用普通循环函数，便于对比和理解）</em><br><br><br>可以在<code>work_threading()</code>和<code>work()</code>中添加<code>sleep()</code>,确保函数调用的按序进行，也就是example1中的<code>sleep()</code>影响全局变量原因；<br><br><br><img src="https://1j0ker.github.io/md_img/threading_val3.jpg" alt="exmple3.md"></p><br>--------------<h3 id="test-1-多线程探测主机"><a href="#test-1-多线程探测主机" class="headerlink" title="test_1:多线程探测主机"></a>test_1:多线程探测主机</h3><p><em>测试一下我所在的ip段</em></p><p><strong>py代码：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#-*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import threading</span><br><span class="line">import time</span><br><span class="line">import os#调用os模块，启动cmd</span><br><span class="line"></span><br><span class="line">code = os.popen(&apos;chcp 65001&apos;)#cmd的GBK编码问题，转成utf-8</span><br><span class="line"></span><br><span class="line">def Check_ip(ip):</span><br><span class="line">    cmd_ip = &apos;ping -n 1 xxx.xxx.xxx.&apos; + str(ip)# -n 1仅仅探测一个回包，速度更快</span><br><span class="line">    #print(cmd_ip)</span><br><span class="line">    check = os.popen(cmd_ip)</span><br><span class="line">    data = check.read()</span><br><span class="line">    #print(data)</span><br><span class="line">    if &apos;TTL&apos; in data:</span><br><span class="line">        print(&apos;%s is UP\n&apos; % cmd_ip)</span><br><span class="line">    time.sleep(0.5)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    for i in range(1,255):</span><br><span class="line">        t = threading.Thread(target=Check_ip, args=(i, ))</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;结果能感觉到，速度明显的加快，打印下进程：</p><p><img src="https://1j0ker.github.io/md_img/threading_checkip1.jpg" alt="exmple4.md"></p><p>###To be continued…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python多线程的变量影响&quot;&gt;&lt;a href=&quot;#Python多线程的变量影响&quot; class=&quot;headerlink&quot; title=&quot;Python多线程的变量影响&quot;&gt;&lt;/a&gt;Python多线程的变量影响&lt;/h1&gt;&lt;h3 id=&quot;example1：sleep-控制
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>X-Forwarded-For 和 X-Real-IP 的一些理解与区别</title>
    <link href="http://yoursite.com/2019/07/15/X-Forward-f/"/>
    <id>http://yoursite.com/2019/07/15/X-Forward-f/</id>
    <published>2019-07-15T14:02:17.000Z</published>
    <updated>2019-07-18T14:45:43.086Z</updated>
    
    <content type="html"><![CDATA[<h1>X-Forwarded-For简单理解</h1><h2> 逐级记录代理信息</h2><p>X-Forwarded-For 是一个 HTTP 扩展头部，用于让web服务器获取真实的用户的IP(但不一定是真实的，可被伪造，比如透明代理)；</p><p><strong>那为什么 Web 服务器只有通过 X-Forwarded-For 头才能获取真实的 IP？</strong><br>&#8195;这里用PHP说明一下， PHP后端获取用户ip会调用$_SERVER[‘REMOTE_ADDR’]，他表示的是和服务器握手的IP是什么(不可伪造)；<br>但假如用户使用代理来访问服务器，那么与服务器建立链接的IP就是 代理ip，并非用户的ip；<br>请求路径可以看做：<br><code>客户端 (--&gt; 正向代理 --&gt; 透明代理 --&gt; 服务器反向代理) --&gt; 服务器端</code><br>( )内的代理不是必须的；<br><br><br><strong>那么什么又是正向代理？透明代理？反向代理？</strong></p><ul><li>正向代理：一般企业在出口网关处设置代理（为了加速，节省流量）</li><li>透明代理：用户为了隐藏自己，自己设置代理（为了翻墙，这样就绕开了企业的正向代理）</li><li>反向代理：部署在Web服务器前面，为了负载均衡，保护内网服务器数据；</li></ul><p>那么下面几种情况：</p><ul><li>假如客户端直接连接 Web 服务器（假设 Web 服务器有公网地址），则$_SERVER[‘REMOTE_ADDR’] 获取到的是客户端的真实 IP ；</li><li>假设 Web 服务器前部署了反向代理（比如 Nginx），则 $_SERVER[‘REMOTE_ADDR’] 获取到的是反向代理设备的 IP（Nginx）；</li><li>假设客户端通过正向代理直接连接 Web 服务器（假设 Web 服务器有公网地址），则 $_SERVER[‘REMOTE_ADDR’] 获取到的正向代理设备的 IP ；</li></ul><p>所以就是由于各种代理的问题，$_SERVER[‘REMOTE_ADDR’] 获取到的 IP 是 Web 服务器 TCP 连接的 IP(不可伪造，web服务器不修改这个ip)，而并非用户真正的ip；</p><p>所以 <strong>X-Forwarded-For</strong>  出现了，它定义的协议头格式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-Forwarded-For: client, proxy1, proxy2</span><br></pre></td></tr></table></figure><p>client 表示用户的真实 IP，每经过一次代理服务器，代理服务器会在这个头里增加用户的 代理IP，但要注意的是，最后一个代理IP是不会添加到这个头里的，而是通过$_SERVER[‘REMOTE_ADDR’] 获取；</p><h3>example_1:</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-Forwarded-For: 1.1.1.1, 2.2.2.2, 3.3.3.3</span><br></pre></td></tr></table></figure><p>原始ip:1.1.1.1 经过了 3 层代理访问当前服务器，第一层代理ip:2.2.2.2, 第二层代理ip:3.3.3.3, 第三层代理ip 也就是和服务端建立请求链接的ip:4.4.4.4;<br><br><br><strong>那接下来详细描述一下XFF的Proxy形成流程：</strong></p><ul><li>用户IP0 ——&gt; Proxy1（IP1）,Proxy1 记录用户的IP0 ，并将请求转发给Proxy2（IP2），并带上了HTTP Header X-Forward-For:IP0 ;</li><li>Proxy2接收到Proxy1的请求后，读取到有X-Forward-For头，那么就将Proxy1的IP1继续添加到X-Forward-For中，构成X-Forward-For:IP0, IP1 ;</li><li>同理，Proxy3 按照第二部构造出 X-Forwarded-For: IP0, IP1, IP2,转发给真正的服务器，比如NGINX，nginx收到了http请求，里面就是 X-Forwarded-For: IP0, IP1, IP2 这样的结果。所以Proxy 3 的IP3，不会出现在这里；</li><li>而Nginx只能通过 $remote_address 获取到，所以 $remote_address 获取的就是和服务器建立TCP链接的IP３（这个无法伪造）；</li></ul><p>很多项目通过获取 X-Forwarded-For 中首个IP作为真实IP。但是X-Forwarded-For可以伪造。</p><h3>Example2:</h3>- **正常访问时：***！！！当X-Forward-For未使用安全设置，或者未使用`Real IP 模块`*时；在proxy1、proxy2、proxy3 的配置中都加上：（实际proxy1、2、3在同一台机器，仅作测试）`proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;`设置打印格式：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_format proxy1 '"[proxy1]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br><span class="line">log_format proxy2 '"[proxy2]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br><span class="line">log_format proxy3 '"[proxy3]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br></pre></td></tr></table></figure><ul><li><code>$remote_addr</code>：与自身建立TCP链接的IP；</li><li><code>$proxy_add_x_forwarded_for</code>：打印XFF规则的IP；</li></ul><p>访问后，打印日志如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 2.2.2.2 123.123.123.123, 1.1.1.1, 2.2.2.2 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>（<strong>123.123.123.123为我的真实IP</strong>）<br>因此，此时取 X-Forwarded-For 中第一个IP得到的确实为客户端真实IP。<br><br></p><ul><li><strong>非正常访问时，在HTTP头中伪造XFF：</strong><br>客户端请求头中人为添加：X-Forwarded-For=192.168.1.1, 192.168.1.2，再看看结果：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 2.2.2.2 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1, 2.2.2.2 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure></li></ul><p>由于 <code>$proxy_add_x_forward_for</code> 会继续追加Proxy1,Proxy2的ip1和ip2，所以真实IP0具体位置在哪里就不好判断了；</p><ul><li><strong>非正常访问，用户自己通过设置正向代理(普通匿名代理，高级匿名代理)的服务器访问目标：</strong><br>例如FQ，VPN之类的代理，访问目标服务器的请求完全由代理服务器发起，X-Forward-For的IP0 也是代理服务器的ip，所以无法判断用户真实ip;<br></li></ul><h2>X-Real-IP</h2><br>是一个自定义头。X-Real-Ip 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP，这个设备可能是其他代理，也可能是真正的请求端。需要注意的是，X-Real-Ip 目前并不属于任何标准，代理和 Web 应用之间可以约定用任何自定义头来传递这个信息。没有相关标准，上面的例子，如果配置了X-Read-IP，可能会有两种情况:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 最后一跳是正向代理，可能会保留真实客户端IP</span><br><span class="line">X-Real-IP: 1.1.1.1(IP0)</span><br><span class="line">// 最后一跳是反向代理，比如Nginx，一般会是与之直接连接的客户端IP</span><br><span class="line">X-Real-IP: 3.3.3.3(IP2)</span><br></pre></td></tr></table></figure><p>所以 ，如果只有一层代理，这两个头的值就是一样的;<br>但是如果有多级代理，x-forwarded-for效果是大于x-real-ip的，x-forwarded-for可以记录完整的代理链路;</p><h3>To be continued...</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;X-Forwarded-For简单理解&lt;/h1&gt;


&lt;h2&gt; 逐级记录代理信息&lt;/h2&gt;

&lt;p&gt;X-Forwarded-For 是一个 HTTP 扩展头部，用于让web服务器获取真实的用户的IP(但不一定是真实的，可被伪造，比如透明代理)；&lt;/p&gt;
&lt;p&gt;&lt;stron
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>UTS安装时问题总结</title>
    <link href="http://yoursite.com/2019/07/12/UTS%E5%AE%89%E8%A3%85%E6%97%B6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/07/12/UTS安装时问题总结/</id>
    <published>2019-07-12T12:39:30.332Z</published>
    <updated>2019-07-11T08:36:35.910Z</updated>
    
    <content type="html"><![CDATA[<p>在学习UTS安装时，会遇到各种各样的问题，安装完成后，对各种出现的问题需要自我总结。</p><h2 id="出现的问题及解决方案"><a href="#出现的问题及解决方案" class="headerlink" title="出现的问题及解决方案"></a>出现的问题及解决方案</h2><h3 id="一、DELL-R730-服务器"><a href="#一、DELL-R730-服务器" class="headerlink" title="一、DELL R730 服务器"></a>一、DELL R730 服务器</h3><h4 id="1-U盘安装centos-7-3："><a href="#1-U盘安装centos-7-3：" class="headerlink" title="1.    U盘安装centos 7.3："></a>1.    U盘安装centos 7.3：</h4><p>a)    DELL R730 F11进入BIOS，设置UEFI后，重启后再次进入F11，BIOS中选择从USB启动，开始安装Centos 7;</p><p>b)    Centos 安装界面时，按e进入编辑，修改成正确的U盘名称：sd*，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /dev#第一次无法查看，失败一次后，命令行查看自己的U盘名称</span><br></pre></td></tr></table></figure><p>安装路径改为 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">… … =hd: /dev/sd* quiet</span><br></pre></td></tr></table></figure><p>c)    Centos 安装过程中，手动分区问题：</p><pre><code>①确认正确的 File system 后，记得Update;②**/home**分区问题：有几块盘，就分几个**/home/sd*** 子目录，最后加上/home目录;</code></pre><p>d)    Centos安装完成后：强制常见User用户，建议系统启动后锁个屏，然后更换为/root用户登录；（防止UTS文件解压后，放入/home下，却发现实际在/home/User下，造成安装脚本的运行失败；</p><h4 id="2-UTS-脚本配置："><a href="#2-UTS-脚本配置：" class="headerlink" title="2.    UTS 脚本配置："></a>2.    UTS 脚本配置：</h4><p>a)    UTS参数配置时，注意绑定的网卡(如<strong>em3</strong>)，后面选择透传网卡时不会再出现在选择列表；</p><p>b)    在运行UTS安装脚本前，建议最大化终端，否则可能会出现分辨率显示不全问题；（在选择透传网卡）;</p><p>c)    脚本安装完成后，提示<strong>reboot</strong>，再确认无error：后再重启；（syn…error脚本中语法错误可忽略）;</p><p>d)    重启后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh list –all#先确定uts_01是否runing</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console uts_01#在开始控制UTS虚拟机</span><br></pre></td></tr></table></figure><hr><h3 id="二、重点注意问题"><a href="#二、重点注意问题" class="headerlink" title="二、重点注意问题"></a>二、重点注意问题</h3><ol><li>在执行UTS参数确定网卡安装脚本后，网卡与网桥绑定，不易卸载网桥；<br>（可进入/<strong>etc/sysconfig/network_config/vibr0</strong>处修改网桥ip）</li></ol><p>2    . 重装虚拟机：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a)virsh shutdown uts_01#关闭当前虚拟机uts_01</span><br><span class="line">virsh undefine uts_01#卸载现有虚拟机uts_01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b)rm –r 现在虚拟机生成的.img#删除uts_01生成的镜像文件；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c)重新执行安装脚本；</span><br></pre></td></tr></table></figure><p>3    . 注意参数配置的，分配内存的大小，太大时，在UTS的web端进行系统或规则包升级时，容易造成卡死。重启UTS虚拟机；</p><p>4    . 安装完成UTS后，web端访问无流量显示：</p><pre><code>a)    查看接口信息，是否接口网桥未开启；b)    联动安装TAM，查看是否已有事件生成，如果有，则是可能是接口显示问题，建议联系开发查看；</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习UTS安装时，会遇到各种各样的问题，安装完成后，对各种出现的问题需要自我总结。&lt;/p&gt;
&lt;h2 id=&quot;出现的问题及解决方案&quot;&gt;&lt;a href=&quot;#出现的问题及解决方案&quot; class=&quot;headerlink&quot; title=&quot;出现的问题及解决方案&quot;&gt;&lt;/a&gt;出现的问题及
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫闯关练习</title>
    <link href="http://yoursite.com/2019/07/11/py-spiders1/"/>
    <id>http://yoursite.com/2019/07/11/py-spiders1/</id>
    <published>2019-07-11T10:02:17.000Z</published>
    <updated>2019-07-12T13:08:00.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python爬虫闯关练习"><a href="#Python爬虫闯关练习" class="headerlink" title="Python爬虫闯关练习"></a>Python爬虫闯关练习</h1><p>黑板客爬虫闯关<a href="http://www.heibanke.com/lesson" target="_blank" rel="noopener">http://www.heibanke.com/lesson</a></p><h2 id="第一关"><a href="#第一关" class="headerlink" title="第一关"></a>第一关</h2><h3 id="连续爬取数据并get传参"><a href="#连续爬取数据并get传参" class="headerlink" title="连续爬取数据并get传参"></a>连续爬取数据并get传参</h3><p>&#8195;<em>先试着输入几次参数，了解题目目标</em><br><img src="https://1j0ker.github.io/md_img/py_zj1.png" alt="exmple1.md"></p><p><strong>思路：</strong>每次输入指定数字后会要求新的get参数，所以需要不断的匹配数字然后提交；<br>    利用模块<code>requests  和    re模块</code><br>这里查看网页源码，可以清晰的看到需要匹配的字段特征：<br><img src="https://1j0ker.github.io/md_img/py_zj2.png" alt="exmple2.md"></p><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex00/'</span></span><br><span class="line">r = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">numbers = re.findall(<span class="string">r'&lt;h3&gt;你需要在网址后输入数字([\d]&#123;5&#125;)'</span>,r.text)</span><br><span class="line"><span class="comment">#print(numbers)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> numbers:</span><br><span class="line">    url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex00/%s/'</span> % numbers[<span class="number">0</span>]</span><br><span class="line">    print(url)</span><br><span class="line">    html = requests.get(url=url)</span><br><span class="line">    numbers = re.findall(<span class="string">r'数字是([\d]&#123;5&#125;)'</span>,html.text)<span class="comment">#通过更新数字，来判断循环的终止</span></span><br><span class="line">    <span class="keyword">print</span> (numbers)</span><br><span class="line"><span class="keyword">else</span>:    </span><br><span class="line">    print(html.text)<span class="comment">#最后过关的页面</span></span><br></pre></td></tr></table></figure><hr><h2 id="最终密码：0第二关"><a href="#最终密码：0第二关" class="headerlink" title="最终密码：0第二关"></a><strong>最终密码：0</strong><br><br><br>第二关</h2><h3 id="POST型传参"><a href="#POST型传参" class="headerlink" title="POST型传参"></a>POST型传参</h3><p><img src="https://1j0ker.github.io/md_img/py_zj3.png" alt="exmple3.md"><br><em>随便输入尝试一下，发现报错信息：</em></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>这里是黑板客爬虫闯关的第二关<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h3</span>&gt;</span>您输入的密码错误, 请重新输入<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br></pre></td></tr></table></figure><p><br><br><br><strong>思路：</strong>正常的登录页面，很明显是<strong>POST</strong>传表单,再加上循环取值就行；<br>同样匹配判断是否包含错误信息，来终止循环；<br>使用模块<code>requests  和    re模块</code></p><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex01/'</span></span><br><span class="line">html = requests.post(url=url, data=&#123;<span class="string">'username'</span>:<span class="string">'ads'</span>,<span class="string">'password'</span>:<span class="number">1</span>&#125;)</span><br><span class="line">char = re.findall(<span class="string">'密码错误'</span>,html.text)</span><br><span class="line">print(char)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">for psd in range(30):</span></span><br><span class="line"><span class="string">    print (psd)</span></span><br><span class="line"><span class="string">    html = requests.post(url=url, data=&#123;'username':'ads','password':psd&#125;)</span></span><br><span class="line"><span class="string">    if '密码错误' not in html.text:</span></span><br><span class="line"><span class="string">        print('password =',psd)</span></span><br><span class="line"><span class="string">        break</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.while使用</span></span><br><span class="line">psd = <span class="number">-2</span></span><br><span class="line"><span class="keyword">while</span> char <span class="keyword">and</span> psd &lt; <span class="number">30</span>:</span><br><span class="line">    <span class="keyword">print</span> (psd)</span><br><span class="line">    html = requests.post(url=url, data=&#123;<span class="string">'username'</span>:<span class="string">'ads'</span>,<span class="string">'password'</span>:psd&#125;)</span><br><span class="line">    char = re.findall(<span class="string">'密码错误'</span>,html.text)</span><br><span class="line">    psd +=<span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'password ='</span>,psd<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><strong>最终密码：4</strong></p><hr><p><br><br></p><h2 id="第三关"><a href="#第三关" class="headerlink" title="第三关"></a>第三关</h2><h3 id="验证登录以及从爬虫保持Token访问"><a href="#验证登录以及从爬虫保持Token访问" class="headerlink" title="验证登录以及从爬虫保持Token访问"></a>验证登录以及从爬虫保持Token访问</h3><p><em>进入第三关的链接<a href="http://www.heibanke.com/lesson/crawler_ex02/" target="_blank" rel="noopener">http://www.heibanke.com/lesson/crawler_ex02/</a><br>但是跳转到了<code>login</code>登陆页面,一开始以为在线的题目出错了，但注册完在登陆进去之后，才明白题目确实是这样的要求：</em><br><img src="https://1j0ker.github.io/md_img/py_zj4.png" alt="exmple4.md"><br>注册登录后，发现题目：<br><img src="https://1j0ker.github.io/md_img/py_zj5.png" alt="exmple5.md"><br><br><br>和第二关很像，用第二关的代码改了下<code>url</code>在运行，出现了问题：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"summary"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Forbidden <span class="tag">&lt;<span class="name">span</span>&gt;</span>(403)<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>CSRF verification failed. Request aborted.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p>发现了访问拒绝，又是<strong>POST</strong>包，这时候要去看一看<strong>request</strong>里面post哪些东西？<br><img src="https://1j0ker.github.io/md_img/py_zj6.png" alt="exmple6.md"><br>很明显多了个<code>token值</code>，再仔细查看，是<strong>request</strong>里cookies的这个值<code>csrftoken=2m7tnWxAV9Bm9lfFjWQ3vvCxAvY3qSxe;</code><br><br><br>那现在问题是这个token值是怎么获得的？经过一番F12是访问题目页面时，<strong>respond</strong>里的<code>Set-Cookie: csrftoken=2m7tnWxAV9Bm9lfFjWQ3vvCxAvY3qSxe;</code>由服务器发送给我们，用来验证接下来我们操作的身份；<br><br><br>也就是说，我们要在<strong>POST</strong>数据时，首先爬取<code>csrftoken</code>值，然后将<code>token</code>一同传过去；</p><hr><p>py代码(这次就没用<code>re</code>模块了)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url_1 = <span class="string">'http://www.heibanke.com/accounts/login'</span></span><br><span class="line">url_2 = <span class="string">'http://www.heibanke.com/lesson/crawler_ex02/'</span></span><br><span class="line">s = requests.Session()  <span class="comment">#创建会话，保存token</span></span><br><span class="line"></span><br><span class="line">s.get(url_1)</span><br><span class="line">token_1 = s.cookies[<span class="string">'csrftoken'</span>]<span class="comment">#get登录，拿到当前token</span></span><br><span class="line">print(token_1)</span><br><span class="line">data_1 = &#123;</span><br><span class="line">    <span class="string">'username'</span>: <span class="string">'efre'</span>,<span class="comment">#注册过的账号</span></span><br><span class="line">    <span class="string">'password'</span>: <span class="string">'123456123456'</span>,</span><br><span class="line">    <span class="string">'csrfmiddlewaretoken'</span>: token_1,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">s.post(url_1,data=data_1)<span class="comment">#登录</span></span><br><span class="line"><span class="comment">#print(r.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_2 = &#123;</span><br><span class="line">    <span class="string">'username'</span>:<span class="string">'admin'</span>,</span><br><span class="line">    <span class="string">'password'</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">'csrfmiddlewaretoken'</span>:<span class="string">''</span>,</span><br><span class="line">    &#125;</span><br><span class="line">s.post(url_2)<span class="comment">#获取题目页面的初始token值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pwd <span class="keyword">in</span> range(<span class="number">31</span>):</span><br><span class="line">    token_2 = s.cookies[<span class="string">'csrftoken'</span>]</span><br><span class="line">    data_2[<span class="string">'csrfmiddlewaretoken'</span>]=token_2</span><br><span class="line">    data_2[<span class="string">'password'</span>]=pwd</span><br><span class="line">    print(pwd,<span class="string">'+'</span>,token_2)<span class="comment">#查看当前密码和当前token值</span></span><br><span class="line">    html = s.post(url_2, data=data_2)</span><br><span class="line">    <span class="comment">#print(html.text)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'密码错误'</span> <span class="keyword">not</span> <span class="keyword">in</span> html.text:</span><br><span class="line">        <span class="comment">#print(html.text)</span></span><br><span class="line">        print(<span class="string">'password ='</span>,pwd)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><strong>最终密码：21</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python爬虫闯关练习&quot;&gt;&lt;a href=&quot;#Python爬虫闯关练习&quot; class=&quot;headerlink&quot; title=&quot;Python爬虫闯关练习&quot;&gt;&lt;/a&gt;Python爬虫闯关练习&lt;/h1&gt;&lt;p&gt;黑板客爬虫闯关&lt;a href=&quot;http://www.hei
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
</feed>
