<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-07-29T14:34:10.186Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>1j0ker</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Crawl_spider模块</title>
    <link href="http://yoursite.com/2019/07/29/crawl_spider/"/>
    <id>http://yoursite.com/2019/07/29/crawl_spider/</id>
    <published>2019-07-29T14:02:17.000Z</published>
    <updated>2019-07-29T14:34:10.186Z</updated>
    
    <content type="html"><![CDATA[<h3>Crawl_spider</h3><p>CrawlSpider也继承自Spider，所以具备它的所有特性，这些特性上章已经讲过了，就再在赘述了，这章就讲点它本身所独有的。<br><code>crawlspider</code>比传统spider更加简便、智能；我们网站的url都是有一定规则的，像django，在<code>view</code>中定义的urls规则就是正则表示的。那么就可以根据这个特性来设计爬虫，而不是每次都要用spider分析页面格式，拆解源码，scrapy提供了<code>CrawlSpider</code>处理此需求。</p><p>看看<code>CrawlSpider</code>类的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrawlSpider</span><span class="params">(Spider)</span>:</span></span><br><span class="line"></span><br><span class="line">    rules = ()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *a, **kw)</span>:</span></span><br><span class="line">        super(CrawlSpider, self).__init__(*a, **kw)</span><br><span class="line">        self._compile_rules()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_results</span><span class="params">(self, response, results)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_request</span><span class="params">(self, rule, link)</span>:</span></span><br><span class="line">        r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">        r.meta.update(rule=rule, link_text=link.text)</span><br><span class="line">        <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_requests_to_follow</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(response, HtmlResponse):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        seen = set()</span><br><span class="line">        <span class="keyword">for</span> n, rule <span class="keyword">in</span> enumerate(self._rules):</span><br><span class="line">            links = [lnk <span class="keyword">for</span> lnk <span class="keyword">in</span> rule.link_extractor.extract_links(response)</span><br><span class="line">                     <span class="keyword">if</span> lnk <span class="keyword">not</span> <span class="keyword">in</span> seen]</span><br><span class="line">            <span class="keyword">if</span> links <span class="keyword">and</span> rule.process_links:</span><br><span class="line">                links = rule.process_links(links)</span><br><span class="line">            <span class="keyword">for</span> link <span class="keyword">in</span> links:</span><br><span class="line">                seen.add(link)</span><br><span class="line">                r = self._build_request(n, link)</span><br><span class="line">                <span class="keyword">yield</span> rule.process_request(r)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_response_downloaded</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        rule = self._rules[response.meta[<span class="string">'rule'</span>]]</span><br><span class="line">        <span class="keyword">return</span> self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_parse_response</span><span class="params">(self, response, callback, cb_kwargs, follow=True)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> callback:</span><br><span class="line">            cb_res = callback(response, **cb_kwargs) <span class="keyword">or</span> ()</span><br><span class="line">            cb_res = self.process_results(response, cb_res)</span><br><span class="line">            <span class="keyword">for</span> requests_or_item <span class="keyword">in</span> iterate_spider_output(cb_res):</span><br><span class="line">                <span class="keyword">yield</span> requests_or_item</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> follow <span class="keyword">and</span> self._follow_links:</span><br><span class="line">            <span class="keyword">for</span> request_or_item <span class="keyword">in</span> self._requests_to_follow(response):</span><br><span class="line">                <span class="keyword">yield</span> request_or_item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_compile_rules</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_method</span><span class="params">(method)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> callable(method):</span><br><span class="line">                <span class="keyword">return</span> method</span><br><span class="line">            <span class="keyword">elif</span> isinstance(method, six.string_types):</span><br><span class="line">                <span class="keyword">return</span> getattr(self, method, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        self._rules = [copy.copy(r) <span class="keyword">for</span> r <span class="keyword">in</span> self.rules]</span><br><span class="line">        <span class="keyword">for</span> rule <span class="keyword">in</span> self._rules:</span><br><span class="line">            rule.callback = get_method(rule.callback)</span><br><span class="line">            rule.process_links = get_method(rule.process_links)</span><br><span class="line">            rule.process_request = get_method(rule.process_request)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler, *args, **kwargs)</span>:</span></span><br><span class="line">        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        spider._follow_links = crawler.settings.getbool(</span><br><span class="line">            <span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> spider</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_crawler</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">        super(CrawlSpider, self).set_crawler(crawler)</span><br><span class="line">        self._follow_links = crawler.settings.getbool(<span class="string">'CRAWLSPIDER_FOLLOW_LINKS'</span>, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>其中的方法很多而且很详细，这里先介绍三种主要常用的方法</strong>：</p><p><strong>rules：</strong><br>它是一个列表，存储的元素时Rule类的实例，其中每一个实例都定义了一种采集站点的行为。如果有多个rule都匹配同一个链接，那么位置下标最小的一个rule将会被使用。 <code>CrawlSpider</code>本身有去重的行为。</p><p><strong>callback：</strong><br>该值可以是一个方法，也可以是一个字符串（spider实例中一个方法的名称）。它就是一个回调方法，它的作用是在此处调用一个指定的方法。<br>但要注意的是：要慎用<code>parse</code>做为回调方法，因为这边的<code>parse</code>已经不像<code>传统spider类</code>中的那样没有具体操作，调用同名的<code>parse</code>可能会造成冲突。</p><p><strong>follow：</strong><br>是一个布尔对象，表示是当前response否继续采集。如果callback是None，那么它就默认为True，否则为False。</p><h4>整个数据的流向如下图所示：</h4><p><img src="https://1j0ker.github.io/md_img/CrawlSpider%E6%B5%81%E7%A8%8B.jpg" alt="CrawlSpider流程_1"></p><br><h3>CrawlSpider爬取微信小程序文章</h3><p><strong>spider.py:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> WXapp.items <span class="keyword">import</span> WxappItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WxappSpiderSpider</span><span class="params">(CrawlSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'wxapp_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'wxapp-union.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.wxapp-union.com/portal.php?mod=list&amp;catid=2&amp;page=1'</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r".+mod=list&amp;catid=2&amp;page=\d"</span>), follow=<span class="literal">True</span>),</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r".+article.+\.html"</span>), callback=<span class="string">"parse_item"</span>, follow=<span class="literal">False</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.xpath(<span class="string">"//h1[@class='ph']/text()"</span>).get()</span><br><span class="line">        author = response.xpath(<span class="string">"//p[@class='authors']//a/text()"</span>).get()</span><br><span class="line">        content_json = response.xpath(<span class="string">"//td[@id='article_content']//text()"</span>).getall()<span class="comment">#很多标签有内容，使用getall()全部获得，list型数据</span></span><br><span class="line">        content = <span class="string">""</span>.join(content_json).strip()<span class="comment">#转成字符串形式</span></span><br><span class="line">        item = WxappItem(title=title, author=author, content=content)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><ul><li><strong>CrawlSpider</strong>使用的重点就是<code>rules规则</code>的构造，确保构造出所允许的采集站点；</li><li>item.py中更改相应的<code>WxappItem</code>方法；</li></ul><br><br>(PS:T-ara十周年快乐啊!)]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3&gt;Crawl_spider&lt;/h3&gt;

&lt;p&gt;CrawlSpider也继承自Spider，所以具备它的所有特性，这些特性上章已经讲过了，就再在赘述了，这章就讲点它本身所独有的。&lt;br&gt;&lt;code&gt;crawlspider&lt;/code&gt;比传统spider更加简便、智能；我们网站
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy框架初学习</title>
    <link href="http://yoursite.com/2019/07/22/scrapy%E5%88%9D%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/07/22/scrapy初学习/</id>
    <published>2019-07-22T14:02:17.000Z</published>
    <updated>2019-07-24T13:38:14.296Z</updated>
    
    <content type="html"><![CDATA[<h3>Scrapy框架</h3>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下：<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_1.jpg" alt="scrapy框架_1"></p><h3> 实例</h3><p><strong>利用scrapy框架爬取糗事百科内容</strong>：</p><ul><li>qsbk.py(spider.py)：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.selector.unified <span class="keyword">import</span> SelectorList</span><br><span class="line"><span class="keyword">from</span> test_1.items <span class="keyword">import</span> Test1Item<span class="comment">#把在items里定义的传输数据类型导入</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.qiushibaike.com/text/page/1/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        duanzidivs = response.xpath(<span class="string">"//div[@id='content-left']/div"</span>)</span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="comment"># print(type(duanzidivs))#返回selectorList类型，可遍历</span></span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="keyword">for</span> duanzidiv <span class="keyword">in</span> duanzidivs:</span><br><span class="line">            <span class="comment">#duanzidiv为selector类型，依旧可以使用xpath</span></span><br><span class="line">            author = duanzidiv.xpath(<span class="string">".//h2/text()"</span>).get().strip()<span class="comment">#.//在当前目录下寻找h2;get()将selector型数据转换成为unicode;strip()去除空格；</span></span><br><span class="line">            content = duanzidiv.xpath(<span class="string">".//div[@class='content']//text()"</span>).getall()<span class="comment">#getall()返回list型数据；</span></span><br><span class="line">            content = <span class="string">""</span>.join(content).strip()<span class="comment">#join()将List型数据转化为字符串</span></span><br><span class="line">            <span class="comment">#print(author, '\n')</span></span><br><span class="line">            <span class="comment">#print(content,'\n')</span></span><br><span class="line">            <span class="comment">#duanzi = &#123;"author":author, "content":content&#125;#创建一个字典</span></span><br><span class="line">            <span class="comment">#yield = duanzi</span></span><br><span class="line">            item = Test1Item(author=author, content=content)<span class="comment">#直接调用定义好的类型</span></span><br><span class="line">            <span class="keyword">yield</span> item<span class="comment">#yield 把普通函数变成生成器；</span></span><br></pre></td></tr></table></figure><p>这里的重点时<code>yield</code>，具体包含了scrapy爬虫的迭代运行 ：<a href="https://blog.csdn.net/qq_40695895/article/details/82882761" title="scrapy中 yield使用详解" target="_blank" rel="noopener">scrapy中 yield使用详解</a>；</p><br><ul><li>利用scrapy的items的实例来传递参数，items.py：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Item</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    author = scrapy.Field()<span class="comment">#将需要传递的数据定义成类型；在spider里导入数据类型；</span></span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure><p> 以及3种存储方式，比较优化，pipelines.py：</p><p> 1.利用json模块，转换item；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> io<span class="comment">#open莫名报错，使用io模块open</span></span><br><span class="line"><span class="comment">#1.利用json模块，转换item；</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">         self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>)<span class="comment">#创建一个duanzi.json文件，写方式，可以在初始化函数中，也可以在open_spider()中</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">         print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = open("duanzi.json",'w',encoding='utf-8')</span></span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">         item_json = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>)<span class="comment">#注意item要先转成程字典形式；把spider中传过来的duanzi转化成为json类型;ensure_ascii=False保存成中文</span></span><br><span class="line">         self.fp.write(item_json+<span class="string">'\n'</span>)<span class="comment">#将duanzi写入文件</span></span><br><span class="line">         <span class="keyword">return</span> item</span><br><span class="line">     <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">         self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">         print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><p>2.使用exporters.JsonItemExporter模块，bytes写入;结果一个列表内放所有字典：先将item数据放入内存中，但item过大时，消耗内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br><span class="line"><span class="comment">#2.使用exporters.JsonItemExporter模块，bytes写入;结果一个列表内放所有字典：先将item数据放入内存中，item过大时，消耗内存</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'wb'</span>)<span class="comment">#创建一个duanzi.json文件，二进制写方式，二进制方法就没有编码了</span></span><br><span class="line">        self.exporter = JsonItemExporter(self.fp,ensure_ascii=<span class="literal">False</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = io.open('duanzi.json','wb')</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">        print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><p>3.使用exporters.JsonLinesItemExporter模块，bytes写入;结果一个字典一行,清晰安全不占用内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonLinesItemExporter</span><br><span class="line"><span class="comment">#3.使用exporters.JsonLinesItemExporter模块，bytes写入;结果一个字典一行</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1Pipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.fp = io.open(<span class="string">'duanzi.json'</span>,<span class="string">'wb'</span>)<span class="comment">#创建一个duanzi.json文件，二进制写方式，二进制方法就没有编码了</span></span><br><span class="line">        self.exporter = JsonLinesItemExporter(self.fp,ensure_ascii=<span class="literal">False</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">"spider开始..."</span>)<span class="comment">#self.fp = io.open('duanzi.json','wb')</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.fp.close()<span class="comment">#关闭文件</span></span><br><span class="line">        print(<span class="string">"spider结束"</span>)</span><br></pre></td></tr></table></figure><ul><li>settings.py的参数不要忘记设置：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span><span class="comment">#关闭查找robottxt页面</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">   <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">   <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">    <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'</span></span><br><span class="line">&#125;<span class="comment">#设置常用头信息</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'test_1.pipelines.Test1Pipeline'</span>: <span class="number">300</span>,<span class="comment">#值越小越先执行</span></span><br><span class="line">&#125;<span class="comment">#开启pipelines</span></span><br></pre></td></tr></table></figure><p><strong>效果：</strong><br>爬取的数据被存放在duanzi.json中：<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_2.jpg" alt="scrapy框架_2"></p><h4>多页面爬取</h4><p>还是寻找页码所在的特征：</p><ul><li>在当前<code>&lt;div&gt;</code>标签下的最后一个<code>&lt;li&gt;</code>里存放着下一页的url;</li><li>在最后一页，也就是13页下，<code>&lt;li&gt;</code>标签里没有存放下一页的链接；<br><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_3.jpg" alt="scrapy框架_3"></li></ul><p><img src="https://1j0ker.github.io/md_img/scrapy%E6%A1%86%E6%9E%B6_4.jpg" alt="scrapy框架_4"></p><p>所以可以通过判断是否能获取到<code>next_url</code>来控制爬取的页数：<br><strong>qsbk.py(spider):</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.selector.unified <span class="keyword">import</span> SelectorList</span><br><span class="line"><span class="keyword">from</span> test_1.items <span class="keyword">import</span> Test1Item<span class="comment">#把在items里定义的传输数据类型导入</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QsbkSpiderSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'qsbk_spider'</span></span><br><span class="line">    allowed_domains = [<span class="string">'qiushibaike.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.qiushibaike.com/text/page/1/'</span>]</span><br><span class="line">    base_domain = <span class="string">'https://www.qiushibaike.com'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        duanzidivs = response.xpath(<span class="string">"//div[@id='content-left']/div"</span>)</span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="comment"># print(type(duanzidivs))#返回selectorList类型，可遍历</span></span><br><span class="line">        <span class="comment"># print('---'*40)</span></span><br><span class="line">        <span class="keyword">for</span> duanzidiv <span class="keyword">in</span> duanzidivs:</span><br><span class="line">            <span class="comment">#duanzidiv为selector类型，依旧可以使用xpath</span></span><br><span class="line">            author = duanzidiv.xpath(<span class="string">".//h2/text()"</span>).get().strip()<span class="comment">#.//在当前目录下寻找h2;get()将selector型数据转换成为unicode;strip()去除空格；</span></span><br><span class="line">            content = duanzidiv.xpath(<span class="string">".//div[@class='content']//text()"</span>).getall()<span class="comment">#getall()返回list型数据；</span></span><br><span class="line">            content = <span class="string">""</span>.join(content).strip()<span class="comment">#join()将List型数据转化为字符串</span></span><br><span class="line">            <span class="comment">#print(author, '\n')</span></span><br><span class="line">            <span class="comment">#print(content,'\n')</span></span><br><span class="line">            <span class="comment">#duanzi = &#123;"author":author, "content":content&#125;#创建一个字典</span></span><br><span class="line">            <span class="comment">#yield = duanzi</span></span><br><span class="line">            item = Test1Item(author=author, content=content)<span class="comment">#直接调用定义好的类型</span></span><br><span class="line">            <span class="keyword">yield</span> item<span class="comment">#yield 把普通函数变成生成器；</span></span><br><span class="line">        next_url = response.xpath(<span class="string">"//ul[@class='pagination']/li[last()]/a/@href"</span>).get()<span class="comment">#取最后一个&lt;li&gt;标签，如果里面没有&lt;a href=''&gt;标签内容了，最说明是最后一页了</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> next_url:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            page_num = self.base_domain+next_url</span><br><span class="line">            print(page_num)</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(page_num,callback=self.parse)<span class="comment">#利用Request中的callback再次调用解析parse(),注意链接双方同类型</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3&gt;Scrapy框架&lt;/h3&gt;
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。
其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Ama
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>CSP的防护与绕过</title>
    <link href="http://yoursite.com/2019/07/20/CSP%E7%9A%84%E9%98%B2%E6%8A%A4%E4%B8%8E%E7%BB%95%E8%BF%87/"/>
    <id>http://yoursite.com/2019/07/20/CSP的防护与绕过/</id>
    <published>2019-07-20T14:02:17.000Z</published>
    <updated>2019-07-22T14:41:53.587Z</updated>
    
    <content type="html"><![CDATA[<h1>CSP的概念</h1><h3> CSP的前端防护</h3>XSS向来是安全方面所重视的一大难题，虽然近年来在OWASP中下降了一些，但仍是实际生产环境中不可忽略的问题。对于一个基本的XSS漏洞页面，它发生的原因往往是从用户输入的数据到输出没有有效的过滤，就比如下面的这个范例代码。<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?php</span></span><br><span class="line">$a = $_GET[<span class="string">'a'</span>];</span><br><span class="line"><span class="keyword">echo</span> $a;</span><br></pre></td></tr></table></figure><p>毫无过滤，我们可以很轻松的构造一种payload来实现XSS：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a=&lt;script&gt;alert(1)&lt;/script&gt;</span><br><span class="line">a=&lt;img/src=1/onerror=alert(1)&gt;</span><br><span class="line">a=&lt;svg/onload=alert(1)&gt;</span><br></pre></td></tr></table></figure><p>所以大多数会使用<code>htmlspecialchars</code>函数来过滤输入，或者更严格一些，使用<code>htmlspecialchars</code> + 黑名单的形式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&amp; (AND) =&gt; &amp;amp;</span><br><span class="line">" (双引号) =&gt; &amp;quot; (当ENT_NOQUOTES没有设置的时候) </span><br><span class="line">' (单引号) =&gt; &amp;#039; (当ENT_QUOTES设置) </span><br><span class="line"><span class="tag">&lt; (小于号) =&gt;</span> &amp;lt; </span><br><span class="line">&gt; (大于号) =&gt; &amp;gt; </span><br><span class="line">...</span><br><span class="line">on\w+=</span><br><span class="line">script</span><br><span class="line">svg</span><br><span class="line">iframe</span><br><span class="line">link</span><br><span class="line">...</span><br><span class="line">% * + , – / ; <span class="tag">&lt; = &gt;</span> ^ | `</span><br></pre></td></tr></table></figure><p>但是实际上的生产环境永远要比以前的多，XSS的插入点，可能是标签的属性，标签的值，标签的头部等等…</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"&#123;插入点&#125;"</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#123;插入点&#125;</span>&gt;</span>(没有引号)</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span>&#123;插入点&#125;<span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>见过的那么多XSS漏洞，大多数漏洞的产生点，都是过滤函数忽略的地方。</strong></p><h4>所以CSP诞生了：</h4>**CSP**是网页安全政策(Content Security Policy)的缩写。是一种由开发者定义的安全性政策申明，通过CSP所约束的责任指定可信的内容来源，（内容可以是指脚本、图片、style 等远程资源）。通过CSP协定，可以防止XSS攻击，让web处一个安全运行的环境中。 CSP 的实质就是白名单制度，开发者明确告诉客户端，哪些外部资源可以加载和执行，等同于提供白名单。它的实现和执行全部由浏览器完成，开发者只需提供配置。CSP 大大增强了网页的安全性。攻击者即使发现了漏洞，也没法注入脚本，除非还控制了一台列入了白名单的可信主机。<br>**特点**就是他是在浏览器层面做的防护，是和同源策略同一级别，除非浏览器本身出现漏洞，否则不可能从机制上绕过。<p>CSP只允许被认可的JS块、JS文件、CSS等解析，只允许向指定的域发起请求。<br>一个简单的 HTTP 头信息CSP规则:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: script-src 'self'; object-src 'none'; style-src cdn.example.org third-party.org; child-src https:");</span><br></pre></td></tr></table></figure><p>其中的规则指令分很多种，每种指令都分管浏览器中请求的一部分，上面代码中，CSP 做了如下配置:</p><ul><li><p>脚本：只信任当前域名</p></li><li><p>object标签：不信任任何URL，即不加载任何资源</p></li><li><p>样式表：只信任cdn.example.org和third-party.org</p></li><li><p>框架（frame）：必须使用HTTPS协议加载</p></li><li><p>其他资源：没有限制</p></li></ul><p>启用后，不符合 CSP 的外部资源就会被阻止加载。<br><strong>详细的指令配置参考</strong>：<a href="https://content-security-policy.com" title="https://content-security-policy.com" target="_blank" rel="noopener">https://content-security-policy.com</a></p><p>简单来说，针对不同来源，不同方式的资源加载，都有相应的加载策略。</p><p>我们可以说，如果一个站点有足够严格的CSP规则，那么XSS or CSRF就可以从根源上被防止。</p><p>但事实真的是这样吗？</p><h3>CSP Bypass</h3>- 最普通最常见的CSP规则，只允许加载当前域的js:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: default-src 'self'; script-src 'self' ");</span><br></pre></td></tr></table></figure><p><strong>但是！</strong>如果站内有上传图片的地方，如果我们上传一个内容为js的图片，图片就在网站的当前域下了：<br>那么直接加载图片：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'upload/test.js'</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><br>- 当你发现设置self并不安全的时候，可能会选择把静态文件的可信域限制到目录，看上去好像没什么问题了：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header(" Content-Security-Policy: default-src 'self '; script-src http://127.0.0.1/static/ ");</span><br></pre></td></tr></table></figure><p>但是如果可信域内存在一个可控的重定向文件，那么CSP的目录限制就可以被绕过。</p><p>假设static目录下存在一个302文件：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Static/302.php</span><br><span class="line"></span><br><span class="line">&lt;?php Header(&quot;location: &quot;.$_GET[&apos;url&apos;])?&gt;</span><br></pre></td></tr></table></figure><p>首先上传一个内容为JS的上传一个test.jpg；<br>然后通过302.php跳转到upload目录加载js就可以成功执行：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"static/302.php?url=upload/test.js"</span>&gt;</span></span><br></pre></td></tr></table></figure><br>- CSP除了阻止不可信js的解析以外，还有一个功能是组织向不可信域的请求:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">header("Content-Security-Policy: default-src 'self'; script-src 'self' ");</span><br></pre></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://lorexxar.cn/1.jpg"</span>&gt;</span> #尝试加载域外图片，被阻止</span><br></pre></td></tr></table></figure><p>但是在CSP1.0中，对于link的限制并不完整，不同浏览器包括chrome和firefox对CSP的支持都不完整，每个浏览器都维护一份包括CSP1.0、部分CSP2.0、少部分CSP3.0的CSP规则：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"prefetch"</span> <span class="attr">href</span>=<span class="string">"http://lorexxar.cn"</span>&gt;</span> #(H5预加载)(only chrome)</span><br><span class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"dns-prefetch"</span> <span class="attr">href</span>=<span class="string">"http://lorexxar.cn"</span>&gt;</span> #（DNS预加载）</span><br></pre></td></tr></table></figure><p>都会造成域外加载；<br><br></p><p>还有很多类似jsonp和CSP相冲突的组件，造成了CSP无法完全的限制安全域，或者说控制安全域内的资源真正来源是否安全… …<br><br><br>当然随着CSP的升级，一些域控问题得到改善，但有些问题还是无法解决：<br>最要命的，比如：<strong>dom xss</strong>;</p><h5>DOM XSS</h5>现代浏览器大多都有缓存机制，但页面中没有修改或者不需要再次请求后台的时候，浏览器就会从缓存中读取页面内容，`location.hash`就是一个典型的例子；<p>如果JS中存在操作location.hash导致的xss，那么这样的攻击请求不会经过后台；<br>或者通过通过CSS选择器来读取页面内容，页面只变化了CSS，构造纯静态的XSS，不过CSP筛选：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">*<span class="selector-attr">[attribute^="a"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=a"</span>)&#125; </span><br><span class="line">*<span class="selector-attr">[attribute^="b"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=b"</span>)&#125; </span><br><span class="line">*<span class="selector-attr">[attribute^="c"]</span>&#123;<span class="attribute">background</span>:<span class="built_in">url</span>(<span class="string">"record?match=c"</span>)&#125; <span class="selector-attr">[...]</span></span><br></pre></td></tr></table></figure><p>当匹配到对应的属性，页面就会发出相应的请求。<br>上面是常见的3种绕过；<br><a href="https://paper.seebug.org/423/#0x03-csp-bypass" title="更多绕过的方法。" target="_blank" rel="noopener">更多绕过方法。</a></p><h3>总结</h3>CSP虽然有着严格的白名单控制，但仍然只能减轻以偷取信息为目的的XSS行为，对于纯粹的破坏行为则无能为力，特别是DOM Based XSS，所以对于防范XSS还是需要数据的输入校验以及数据输出的Encode，前端浏览器与后端Nginx，Apache的共同协作；<h5>ref:</h5>[1] [https://paper.seebug.org/91/](https://paper.seebug.org/91/ "https://paper.seebug.org/91")<p>[2] <a href="https://lorexxar.cn" title="https://lorexxar.cn" target="_blank" rel="noopener">https://lorexxar.cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;CSP的概念&lt;/h1&gt;


&lt;h3&gt; CSP的前端防护&lt;/h3&gt;
XSS向来是安全方面所重视的一大难题，虽然近年来在OWASP中下降了一些，但仍是实际生产环境中不可忽略的问题。
对于一个基本的XSS漏洞页面，它发生的原因往往是从用户输入的数据到输出没有有效的过滤，就比如下
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>寻找 X-Forwarded-For 的真实IP的一些理解(续)</title>
    <link href="http://yoursite.com/2019/07/17/XFFsearch_real_ip/"/>
    <id>http://yoursite.com/2019/07/17/XFFsearch_real_ip/</id>
    <published>2019-07-17T14:02:17.000Z</published>
    <updated>2019-07-18T15:01:27.565Z</updated>
    
    <content type="html"><![CDATA[<br><h1>寻找 X-Forwarded-For 的真实IP</h1><br>*前言：为什么要寻找真实IP？*因为在实际项目中，用户ip的获取很重要。通过报障用户的ip来快速定位用户的请求日志，在威胁分析中定位攻击源目IP。<br>紧接上次的X-Forward-For的介绍，我们能了解到，XFF头中可能会存在真实IP，不真实的原因总结为两点：<ul><li>人为修改HTTP包头的X-Forward-For,导致真实IP的混杂；</li><li>用户的正向代理访问：这个暂时没有解决办法，而且也是我在做威胁分析时遇到的异常头疼的问题之一；</li></ul><p>所以，接下来，我所讨论的就是，当人为去修改XFF头时，我们能有什么办法去寻找到真实IP，或者说去防止这种情况的发生。</p><hr><br><h3>如何获得真实IP？</h3><br><ul><li><h4>使用 X-Forwarded-For + Real IP 模块:</h4>可以使用nginx的 `ngx_http_realip_module` 模块，从 X-Forwarded-For 或其他属性中提取真实IP。此处以 X-Forwarded-For 结合该模块为例子，需要做两件事：</li><li>请求途径的各代理需要设置 <code>proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for</code>;</li><li>利用 realip 模块获取真实IP<br>这里proxy3的部分配置(proxy3将请求直接转发到后端服务)，如下：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  ...</span><br><span class="line">    location / &#123;</span><br><span class="line">        set_real_ip_from ip1(proxy1的IP); </span><br><span class="line">        real_ip_header    X-Forwarded-For;</span><br><span class="line">        real_ip_recursive on;</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>set_real_ip_from</code>: 表示从何处获取真实IP(解决安全问题，只认可自己信赖的IP)，可以是IP或子网等， 可以设置多个set_real_ip_from，上面的例子中就设置的时Proxy1的IP；</li><li><code>real_ip_header</code>：表示从HTTP包头的哪个header属性中获取真实IP；</li><li><code>real_ip_recursive</code>：递归检索真实IP，若从 X-Forwarded-For 中获取，则需递归检索；若像从X-Real-IP中获取，则无需递归。</li></ul><p>基于上一章日志测试格式的测试数据，试验结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1(IP1) 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1, 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>这里解释一下[Proxy3]中的第一个IP和最后一个IP为什么是我们的真实IP了：</p><ul><li>我们在Proxy3里的<code>set_real_ip_from</code>设置了真实IP的获取来源：Proxy1，而Proxy1 <code>$remote_addr</code>(再次强调，这个不可被更改)的就是我们的真实IP；</li><li>而且在Proxy1中根据XFF的规则，此时用户的代理IP就是他本身的IP；</li></ul><p>最终，proxy3 的 <code>$remote_addr</code> 也拿到了客户端的真实IP 123.123.123.123，然后 proxy3 将 remote_addr 传递到后端服务中去；<br><br></p><ul><li><h4>使用X-Forwarded-For + 安全设置:</h4>由于客户端可以自行传递X-Forwarded-For，因此，可以在第一个代理处重置其值，达到忽略客户端传递的X-Forwarded-For的效果，因为proxy1的 `$remote_addr` 是客户端真实IP:</li></ul><p>在 proxy1 中进行如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_set_header X-Forwarded-For $remote_addr;</span><br></pre></td></tr></table></figure><p>与此类似的是设置Proxy1中 X-Real-IP:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">proxy_set_header X-Real-IP $remote_addr;</span><br></pre></td></tr></table></figure><p>配置下测试日志格式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_format proxy1 '"[proxy1]" $http_x_real_ip "$request" $status';</span><br><span class="line">log_format proxy2 '"[proxy2]" $http_x_real_ip "$request" $status';</span><br><span class="line">log_format proxy3 '"[proxy3]" $http_x_real_ip "$request" $status';</span><br></pre></td></tr></table></figure><p>看一下打印结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" - "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 123.123.123.123 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>在最后的Proxy3中也打印出了真实IP：123.123.123.123</p><hr><br><h2>网站使用CDN之后如何禁止恶意用户真实IP</h2><br>这是前几天做威胁分析时遇到的麻烦问题；在威胁分析中，各种层出不断的恶意扫描、拉取、注入等图谋不轨行为令人头大，当然我们一般可以直接通过 iptables 、 Nginx 的 deny 指令或者是程序来 ban 掉这些恶意请求IP；但是对于套了一层 CDN 或代理的网站，这些方法可能就失效了。尤其是个人网站，可能就一台 VPS，然后套一个免费的 CDN 就行走在互联网了。并不是每个 CDN 都能精准的拦截各种恶意请求的，更闹心的是很多 CDN 还不支持用户在 CDN 上添加 BAN 规则...<p>基于上篇文章的内容的基础上，当网站部署了CDN后，用户又是何种访问模式呐？</p><ul><li>用户访问使用了 CDN 的网站<br>浏览器 –&gt; DNS 解析 –&gt; CDN 节点 –&gt; WEB 数据处理 –&gt; 数据吐到浏览器渲染展示</li><li>用户通过代理上网访问了我们的网站<br>浏览器 –&gt; 代理上网 –&gt; DNS 解析 –&gt; CDN 节点 –&gt; WEB 数据处理 –&gt; 数据吐到浏览器渲染展示</li></ul><p>对于这两种访问模式，直接的<code>iptables -I INPUT -s 用户的ip -j DROP</code> 或者 <code>deny</code>都失去了作用，因为 iptables 和 deny 都只能针对直连 IP，而这2种模式中，WEB 服务器直连 IP 是 CDN 节点或者代理服务器，此时使用 iptable 或 deny 就只能把 CDN 节点 或代理 IP 给封了，可能误杀一大片正常用户了，而真正的罪魁祸首轻轻松松换一个代理 IP 又能继续请求了。</p><p><strong>所以如何获取真实IP非常关键</strong><br>其实也就是利用Nginx上的http模块操作 <code>$http_x_forwarded_for</code>和<code>$clientRealIp</code>:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>获取用户真实IP，并赋值给变量$clientRealIP</span><br><span class="line">map $http_x_forwarded_for  $clientRealIp &#123;</span><br><span class="line">        ""      $remote_addr;</span><br><span class="line">        ~^(?P&lt;firstAddr&gt;[0-9\.]+),?.*$  $firstAddr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实也就是上面讲到的，匹配了 <code>$http_x_forwarded_for</code> 的第一个值(安全设置下的X-Forward-For第一个值就为源IP)，而且代码中还配合使用了 <code>$remote_addr</code>，避免了直接访问下时 <code>$http_x_forwarded_for</code>为空；替代了传统的 <code>$remote_addr</code>；</p><h4>其实都是对用户未使用高度匿名代理、普通匿名代理的处理</h4> <p><a href="https://blog.csdn.net/bytxl/article/details/15334153" title="3种正向代理方式介绍" target="_blank" rel="noopener">3种正向代理方式的介绍</a></p><p>既然已经拿到了真实 IP，利用Nginx的判断，<code>return 403</code>就好了；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>如果真实IP为 123.123.123.123，那么返回403</span><br><span class="line">if ($clientRealIp ~* "121.42.0.18|121.42.0.19") &#123;</span><br><span class="line">        #add_header Content-Type text/plain;</span><br><span class="line">        return 403;</span><br><span class="line">        break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>到这里我对<code>X-Forward-For</code>的一些理解就介绍完了，后面还有对3种代理方式的学习，以及透明代理，普通匿名代理和高级匿名代理的理解；<br>贴一下参考：<a href="https://blog.51cto.com/z00w00/1031287" title="透明代理、正向代理、反向代理" target="_blank" rel="noopener">透明代理、正向代理、反向代理</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;br&gt;
&lt;h1&gt;寻找 X-Forwarded-For 的真实IP&lt;/h1&gt;
&lt;br&gt;
*前言：为什么要寻找真实IP？*
因为在实际项目中，用户ip的获取很重要。通过报障用户的ip来快速定位用户的请求日志，在威胁分析中定位攻击源目IP。
&lt;br&gt;
紧接上次的X-Forward-F
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>X-Forwarded-For 和 X-Real-IP 的一些理解与区别</title>
    <link href="http://yoursite.com/2019/07/15/X-Forward-f/"/>
    <id>http://yoursite.com/2019/07/15/X-Forward-f/</id>
    <published>2019-07-15T14:02:17.000Z</published>
    <updated>2019-07-18T14:45:43.086Z</updated>
    
    <content type="html"><![CDATA[<h1>X-Forwarded-For简单理解</h1><h2> 逐级记录代理信息</h2><p>X-Forwarded-For 是一个 HTTP 扩展头部，用于让web服务器获取真实的用户的IP(但不一定是真实的，可被伪造，比如透明代理)；</p><p><strong>那为什么 Web 服务器只有通过 X-Forwarded-For 头才能获取真实的 IP？</strong><br>&#8195;这里用PHP说明一下， PHP后端获取用户ip会调用$_SERVER[‘REMOTE_ADDR’]，他表示的是和服务器握手的IP是什么(不可伪造)；<br>但假如用户使用代理来访问服务器，那么与服务器建立链接的IP就是 代理ip，并非用户的ip；<br>请求路径可以看做：<br><code>客户端 (--&gt; 正向代理 --&gt; 透明代理 --&gt; 服务器反向代理) --&gt; 服务器端</code><br>( )内的代理不是必须的；<br><br><br><strong>那么什么又是正向代理？透明代理？反向代理？</strong></p><ul><li>正向代理：一般企业在出口网关处设置代理（为了加速，节省流量）</li><li>透明代理：用户为了隐藏自己，自己设置代理（为了翻墙，这样就绕开了企业的正向代理）</li><li>反向代理：部署在Web服务器前面，为了负载均衡，保护内网服务器数据；</li></ul><p>那么下面几种情况：</p><ul><li>假如客户端直接连接 Web 服务器（假设 Web 服务器有公网地址），则$_SERVER[‘REMOTE_ADDR’] 获取到的是客户端的真实 IP ；</li><li>假设 Web 服务器前部署了反向代理（比如 Nginx），则 $_SERVER[‘REMOTE_ADDR’] 获取到的是反向代理设备的 IP（Nginx）；</li><li>假设客户端通过正向代理直接连接 Web 服务器（假设 Web 服务器有公网地址），则 $_SERVER[‘REMOTE_ADDR’] 获取到的正向代理设备的 IP ；</li></ul><p>所以就是由于各种代理的问题，$_SERVER[‘REMOTE_ADDR’] 获取到的 IP 是 Web 服务器 TCP 连接的 IP(不可伪造，web服务器不修改这个ip)，而并非用户真正的ip；</p><p>所以 <strong>X-Forwarded-For</strong>  出现了，它定义的协议头格式：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-Forwarded-For: client, proxy1, proxy2</span><br></pre></td></tr></table></figure><p>client 表示用户的真实 IP，每经过一次代理服务器，代理服务器会在这个头里增加用户的 代理IP，但要注意的是，最后一个代理IP是不会添加到这个头里的，而是通过$_SERVER[‘REMOTE_ADDR’] 获取；</p><h3>example_1:</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-Forwarded-For: 1.1.1.1, 2.2.2.2, 3.3.3.3</span><br></pre></td></tr></table></figure><p>原始ip:1.1.1.1 经过了 3 层代理访问当前服务器，第一层代理ip:2.2.2.2, 第二层代理ip:3.3.3.3, 第三层代理ip 也就是和服务端建立请求链接的ip:4.4.4.4;<br><br><br><strong>那接下来详细描述一下XFF的Proxy形成流程：</strong></p><ul><li>用户IP0 ——&gt; Proxy1（IP1）,Proxy1 记录用户的IP0 ，并将请求转发给Proxy2（IP2），并带上了HTTP Header X-Forward-For:IP0 ;</li><li>Proxy2接收到Proxy1的请求后，读取到有X-Forward-For头，那么就将Proxy1的IP1继续添加到X-Forward-For中，构成X-Forward-For:IP0, IP1 ;</li><li>同理，Proxy3 按照第二部构造出 X-Forwarded-For: IP0, IP1, IP2,转发给真正的服务器，比如NGINX，nginx收到了http请求，里面就是 X-Forwarded-For: IP0, IP1, IP2 这样的结果。所以Proxy 3 的IP3，不会出现在这里；</li><li>而Nginx只能通过 $remote_address 获取到，所以 $remote_address 获取的就是和服务器建立TCP链接的IP３（这个无法伪造）；</li></ul><p>很多项目通过获取 X-Forwarded-For 中首个IP作为真实IP。但是X-Forwarded-For可以伪造。</p><h3>Example2:</h3>- **正常访问时：***！！！当X-Forward-For未使用安全设置，或者未使用`Real IP 模块`*时；在proxy1、proxy2、proxy3 的配置中都加上：（实际proxy1、2、3在同一台机器，仅作测试）`proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;`设置打印格式：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_format proxy1 '"[proxy1]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br><span class="line">log_format proxy2 '"[proxy2]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br><span class="line">log_format proxy3 '"[proxy3]" $remote_addr $proxy_add_x_forwarded_for "$request" $status';</span><br></pre></td></tr></table></figure><ul><li><code>$remote_addr</code>：与自身建立TCP链接的IP；</li><li><code>$proxy_add_x_forwarded_for</code>：打印XFF规则的IP；</li></ul><p>访问后，打印日志如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 2.2.2.2 123.123.123.123, 1.1.1.1, 2.2.2.2 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure><p>（<strong>123.123.123.123为我的真实IP</strong>）<br>因此，此时取 X-Forwarded-For 中第一个IP得到的确实为客户端真实IP。<br><br></p><ul><li><strong>非正常访问时，在HTTP头中伪造XFF：</strong><br>客户端请求头中人为添加：X-Forwarded-For=192.168.1.1, 192.168.1.2，再看看结果：<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">"[proxy1]" 123.123.123.123 192.168.1.1, 192.168.1.2, 123.123.123.123 "GET /hello HTTP/1.1" 200</span><br><span class="line">"[proxy2]" 1.1.1.1 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1 "GET /hello HTTP/1.0" 200</span><br><span class="line">"[proxy3]" 2.2.2.2 192.168.1.1, 192.168.1.2, 123.123.123.123, 1.1.1.1, 2.2.2.2 "GET /hello HTTP/1.0" 200</span><br></pre></td></tr></table></figure></li></ul><p>由于 <code>$proxy_add_x_forward_for</code> 会继续追加Proxy1,Proxy2的ip1和ip2，所以真实IP0具体位置在哪里就不好判断了；</p><ul><li><strong>非正常访问，用户自己通过设置正向代理(普通匿名代理，高级匿名代理)的服务器访问目标：</strong><br>例如FQ，VPN之类的代理，访问目标服务器的请求完全由代理服务器发起，X-Forward-For的IP0 也是代理服务器的ip，所以无法判断用户真实ip;<br></li></ul><h2>X-Real-IP</h2><br>是一个自定义头。X-Real-Ip 通常被 HTTP 代理用来表示与它产生 TCP 连接的设备 IP，这个设备可能是其他代理，也可能是真正的请求端。需要注意的是，X-Real-Ip 目前并不属于任何标准，代理和 Web 应用之间可以约定用任何自定义头来传递这个信息。没有相关标准，上面的例子，如果配置了X-Read-IP，可能会有两种情况:<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 最后一跳是正向代理，可能会保留真实客户端IP</span><br><span class="line">X-Real-IP: 1.1.1.1(IP0)</span><br><span class="line">// 最后一跳是反向代理，比如Nginx，一般会是与之直接连接的客户端IP</span><br><span class="line">X-Real-IP: 3.3.3.3(IP2)</span><br></pre></td></tr></table></figure><p>所以 ，如果只有一层代理，这两个头的值就是一样的;<br>但是如果有多级代理，x-forwarded-for效果是大于x-real-ip的，x-forwarded-for可以记录完整的代理链路;</p><h3>To be continued...</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;X-Forwarded-For简单理解&lt;/h1&gt;


&lt;h2&gt; 逐级记录代理信息&lt;/h2&gt;

&lt;p&gt;X-Forwarded-For 是一个 HTTP 扩展头部，用于让web服务器获取真实的用户的IP(但不一定是真实的，可被伪造，比如透明代理)；&lt;/p&gt;
&lt;p&gt;&lt;stron
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>Python多线程的变量影响</title>
    <link href="http://yoursite.com/2019/07/15/py_threading/"/>
    <id>http://yoursite.com/2019/07/15/py_threading/</id>
    <published>2019-07-15T14:02:17.000Z</published>
    <updated>2019-07-16T15:09:18.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python多线程的变量影响"><a href="#Python多线程的变量影响" class="headerlink" title="Python多线程的变量影响"></a>Python多线程的变量影响</h1><h3 id="example1：sleep-控制多线程的变量"><a href="#example1：sleep-控制多线程的变量" class="headerlink" title="example1：sleep()控制多线程的变量"></a>example1：sleep()控制多线程的变量</h3><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">num = <span class="number">100</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> num</span><br><span class="line">    print(<span class="string">"work_1 ---- %d\n"</span> % num)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"work_2 ---- %d\n"</span> % num)</span><br><span class="line">        time.sleep(<span class="number">1</span>)<span class="comment">#sleep()控制了work_2线程顺序</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"num最初：%d\n"</span> %num)</span><br><span class="line">    t_1 = threading.Thread(target=work_1)</span><br><span class="line">    t_2 = threading.Thread(target=work_2)</span><br><span class="line">    t_2.start()</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)<span class="comment">#sleep()控制了线程顺序</span></span><br><span class="line">    t_1.start()<span class="comment">#线程只能start一次</span></span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line">    print(<span class="string">"num最后：%d\n"</span> %num)</span><br></pre></td></tr></table></figure><p>通过运行结果，发现<code>work_2()</code>中的sleep(1)使得work_2的线程未执行完函数，num便被<code>work_1()</code>调用，所以work_1的num不是103；<br><br><br><img src="https://1j0ker.github.io/md_img/threading_1.jpg" alt="exmple1.md"><br><br></p><h3 id="example2：多线程各自非顺序计算对全局变量的影响"><a href="#example2：多线程各自非顺序计算对全局变量的影响" class="headerlink" title="example2：多线程各自非顺序计算对全局变量的影响"></a>example2：多线程各自非顺序计算对全局变量的影响</h3><p><br>—————————————————————————-<br><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">g_num = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work_threading</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> g_num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        g_num += <span class="number">1</span></span><br><span class="line">     </span><br><span class="line">    print(<span class="string">"work_threading ---- %d\n"</span> % g_num)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> g_num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        g_num += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    print(<span class="string">"work ---- %d\n"</span> % g_num)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">print(<span class="string">"创建多线程之前g_num: %d"</span>%g_num)</span><br><span class="line"></span><br><span class="line">t = threading.Thread(target=work_threading, args=(<span class="number">100000</span>,))</span><br><span class="line">t.start()</span><br><span class="line"><span class="comment">#time.sleep(1)</span></span><br><span class="line">work(<span class="number">100000</span>)</span><br></pre></td></tr></table></figure><p><em>看一下执行结果：</em></p><p><img src="https://1j0ker.github.io/md_img/threading_val2.jpg" alt="exmple2.md"><br><br><br>&#8195;&#8195;能看到，每次执行结果都可能都不相同，原因就是，<code>work_threading()</code>线程调用时使其独自计算，代码继续下读，到<code>work()</code>导致在未完成完整的函数计算时，被双方调用的g_num可能是非最终结果（正常计算应该是100000），造成了最后参数的混乱不一；<br><em>（<code>work()</code>也可以是多线程函数，这里用普通循环函数，便于对比和理解）</em><br><br><br>可以在<code>work_threading()</code>和<code>work()</code>中添加<code>sleep()</code>,确保函数调用的按序进行，也就是example1中的<code>sleep()</code>影响全局变量原因；<br><br><br><img src="https://1j0ker.github.io/md_img/threading_val3.jpg" alt="exmple3.md"></p><br>--------------<h3 id="test-1-多线程探测主机"><a href="#test-1-多线程探测主机" class="headerlink" title="test_1:多线程探测主机"></a>test_1:多线程探测主机</h3><p><em>测试一下我所在的ip段</em></p><p><strong>py代码：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#-*- coding:utf-8 -*-</span><br><span class="line"></span><br><span class="line">import threading</span><br><span class="line">import time</span><br><span class="line">import os#调用os模块，启动cmd</span><br><span class="line"></span><br><span class="line">code = os.popen(&apos;chcp 65001&apos;)#cmd的GBK编码问题，转成utf-8</span><br><span class="line"></span><br><span class="line">def Check_ip(ip):</span><br><span class="line">    cmd_ip = &apos;ping -n 1 xxx.xxx.xxx.&apos; + str(ip)# -n 1仅仅探测一个回包，速度更快</span><br><span class="line">    #print(cmd_ip)</span><br><span class="line">    check = os.popen(cmd_ip)</span><br><span class="line">    data = check.read()</span><br><span class="line">    #print(data)</span><br><span class="line">    if &apos;TTL&apos; in data:</span><br><span class="line">        print(&apos;%s is UP\n&apos; % cmd_ip)</span><br><span class="line">    time.sleep(0.5)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    for i in range(1,255):</span><br><span class="line">        t = threading.Thread(target=Check_ip, args=(i, ))</span><br><span class="line">        t.start()</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>&#8195;&#8195;结果能感觉到，速度明显的加快，打印下进程：</p><p><img src="https://1j0ker.github.io/md_img/threading_checkip1.jpg" alt="exmple4.md"></p><p>###To be continued…</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python多线程的变量影响&quot;&gt;&lt;a href=&quot;#Python多线程的变量影响&quot; class=&quot;headerlink&quot; title=&quot;Python多线程的变量影响&quot;&gt;&lt;/a&gt;Python多线程的变量影响&lt;/h1&gt;&lt;h3 id=&quot;example1：sleep-控制
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
  <entry>
    <title>UTS安装时问题总结</title>
    <link href="http://yoursite.com/2019/07/12/UTS%E5%AE%89%E8%A3%85%E6%97%B6%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2019/07/12/UTS安装时问题总结/</id>
    <published>2019-07-12T12:39:30.332Z</published>
    <updated>2019-07-11T08:36:35.910Z</updated>
    
    <content type="html"><![CDATA[<p>在学习UTS安装时，会遇到各种各样的问题，安装完成后，对各种出现的问题需要自我总结。</p><h2 id="出现的问题及解决方案"><a href="#出现的问题及解决方案" class="headerlink" title="出现的问题及解决方案"></a>出现的问题及解决方案</h2><h3 id="一、DELL-R730-服务器"><a href="#一、DELL-R730-服务器" class="headerlink" title="一、DELL R730 服务器"></a>一、DELL R730 服务器</h3><h4 id="1-U盘安装centos-7-3："><a href="#1-U盘安装centos-7-3：" class="headerlink" title="1.    U盘安装centos 7.3："></a>1.    U盘安装centos 7.3：</h4><p>a)    DELL R730 F11进入BIOS，设置UEFI后，重启后再次进入F11，BIOS中选择从USB启动，开始安装Centos 7;</p><p>b)    Centos 安装界面时，按e进入编辑，修改成正确的U盘名称：sd*，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /dev#第一次无法查看，失败一次后，命令行查看自己的U盘名称</span><br></pre></td></tr></table></figure><p>安装路径改为 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">… … =hd: /dev/sd* quiet</span><br></pre></td></tr></table></figure><p>c)    Centos 安装过程中，手动分区问题：</p><pre><code>①确认正确的 File system 后，记得Update;②**/home**分区问题：有几块盘，就分几个**/home/sd*** 子目录，最后加上/home目录;</code></pre><p>d)    Centos安装完成后：强制常见User用户，建议系统启动后锁个屏，然后更换为/root用户登录；（防止UTS文件解压后，放入/home下，却发现实际在/home/User下，造成安装脚本的运行失败；</p><h4 id="2-UTS-脚本配置："><a href="#2-UTS-脚本配置：" class="headerlink" title="2.    UTS 脚本配置："></a>2.    UTS 脚本配置：</h4><p>a)    UTS参数配置时，注意绑定的网卡(如<strong>em3</strong>)，后面选择透传网卡时不会再出现在选择列表；</p><p>b)    在运行UTS安装脚本前，建议最大化终端，否则可能会出现分辨率显示不全问题；（在选择透传网卡）;</p><p>c)    脚本安装完成后，提示<strong>reboot</strong>，再确认无error：后再重启；（syn…error脚本中语法错误可忽略）;</p><p>d)    重启后：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh list –all#先确定uts_01是否runing</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console uts_01#在开始控制UTS虚拟机</span><br></pre></td></tr></table></figure><hr><h3 id="二、重点注意问题"><a href="#二、重点注意问题" class="headerlink" title="二、重点注意问题"></a>二、重点注意问题</h3><ol><li>在执行UTS参数确定网卡安装脚本后，网卡与网桥绑定，不易卸载网桥；<br>（可进入/<strong>etc/sysconfig/network_config/vibr0</strong>处修改网桥ip）</li></ol><p>2    . 重装虚拟机：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a)virsh shutdown uts_01#关闭当前虚拟机uts_01</span><br><span class="line">virsh undefine uts_01#卸载现有虚拟机uts_01</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b)rm –r 现在虚拟机生成的.img#删除uts_01生成的镜像文件；</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c)重新执行安装脚本；</span><br></pre></td></tr></table></figure><p>3    . 注意参数配置的，分配内存的大小，太大时，在UTS的web端进行系统或规则包升级时，容易造成卡死。重启UTS虚拟机；</p><p>4    . 安装完成UTS后，web端访问无流量显示：</p><pre><code>a)    查看接口信息，是否接口网桥未开启；b)    联动安装TAM，查看是否已有事件生成，如果有，则是可能是接口显示问题，建议联系开发查看；</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在学习UTS安装时，会遇到各种各样的问题，安装完成后，对各种出现的问题需要自我总结。&lt;/p&gt;
&lt;h2 id=&quot;出现的问题及解决方案&quot;&gt;&lt;a href=&quot;#出现的问题及解决方案&quot; class=&quot;headerlink&quot; title=&quot;出现的问题及解决方案&quot;&gt;&lt;/a&gt;出现的问题及
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python爬虫闯关练习</title>
    <link href="http://yoursite.com/2019/07/11/py-spiders1/"/>
    <id>http://yoursite.com/2019/07/11/py-spiders1/</id>
    <published>2019-07-11T10:02:17.000Z</published>
    <updated>2019-07-12T13:08:00.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python爬虫闯关练习"><a href="#Python爬虫闯关练习" class="headerlink" title="Python爬虫闯关练习"></a>Python爬虫闯关练习</h1><p>黑板客爬虫闯关<a href="http://www.heibanke.com/lesson" target="_blank" rel="noopener">http://www.heibanke.com/lesson</a></p><h2 id="第一关"><a href="#第一关" class="headerlink" title="第一关"></a>第一关</h2><h3 id="连续爬取数据并get传参"><a href="#连续爬取数据并get传参" class="headerlink" title="连续爬取数据并get传参"></a>连续爬取数据并get传参</h3><p>&#8195;<em>先试着输入几次参数，了解题目目标</em><br><img src="https://1j0ker.github.io/md_img/py_zj1.png" alt="exmple1.md"></p><p><strong>思路：</strong>每次输入指定数字后会要求新的get参数，所以需要不断的匹配数字然后提交；<br>    利用模块<code>requests  和    re模块</code><br>这里查看网页源码，可以清晰的看到需要匹配的字段特征：<br><img src="https://1j0ker.github.io/md_img/py_zj2.png" alt="exmple2.md"></p><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex00/'</span></span><br><span class="line">r = requests.get(url=url)</span><br><span class="line"></span><br><span class="line">numbers = re.findall(<span class="string">r'&lt;h3&gt;你需要在网址后输入数字([\d]&#123;5&#125;)'</span>,r.text)</span><br><span class="line"><span class="comment">#print(numbers)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> numbers:</span><br><span class="line">    url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex00/%s/'</span> % numbers[<span class="number">0</span>]</span><br><span class="line">    print(url)</span><br><span class="line">    html = requests.get(url=url)</span><br><span class="line">    numbers = re.findall(<span class="string">r'数字是([\d]&#123;5&#125;)'</span>,html.text)<span class="comment">#通过更新数字，来判断循环的终止</span></span><br><span class="line">    <span class="keyword">print</span> (numbers)</span><br><span class="line"><span class="keyword">else</span>:    </span><br><span class="line">    print(html.text)<span class="comment">#最后过关的页面</span></span><br></pre></td></tr></table></figure><hr><h2 id="最终密码：0第二关"><a href="#最终密码：0第二关" class="headerlink" title="最终密码：0第二关"></a><strong>最终密码：0</strong><br><br><br>第二关</h2><h3 id="POST型传参"><a href="#POST型传参" class="headerlink" title="POST型传参"></a>POST型传参</h3><p><img src="https://1j0ker.github.io/md_img/py_zj3.png" alt="exmple3.md"><br><em>随便输入尝试一下，发现报错信息：</em></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>这里是黑板客爬虫闯关的第二关<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h3</span>&gt;</span>您输入的密码错误, 请重新输入<span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br></pre></td></tr></table></figure><p><br><br><br><strong>思路：</strong>正常的登录页面，很明显是<strong>POST</strong>传表单,再加上循环取值就行；<br>同样匹配判断是否包含错误信息，来终止循环；<br>使用模块<code>requests  和    re模块</code></p><hr><p><strong>py代码：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://www.heibanke.com/lesson/crawler_ex01/'</span></span><br><span class="line">html = requests.post(url=url, data=&#123;<span class="string">'username'</span>:<span class="string">'ads'</span>,<span class="string">'password'</span>:<span class="number">1</span>&#125;)</span><br><span class="line">char = re.findall(<span class="string">'密码错误'</span>,html.text)</span><br><span class="line">print(char)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">for psd in range(30):</span></span><br><span class="line"><span class="string">    print (psd)</span></span><br><span class="line"><span class="string">    html = requests.post(url=url, data=&#123;'username':'ads','password':psd&#125;)</span></span><br><span class="line"><span class="string">    if '密码错误' not in html.text:</span></span><br><span class="line"><span class="string">        print('password =',psd)</span></span><br><span class="line"><span class="string">        break</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2.while使用</span></span><br><span class="line">psd = <span class="number">-2</span></span><br><span class="line"><span class="keyword">while</span> char <span class="keyword">and</span> psd &lt; <span class="number">30</span>:</span><br><span class="line">    <span class="keyword">print</span> (psd)</span><br><span class="line">    html = requests.post(url=url, data=&#123;<span class="string">'username'</span>:<span class="string">'ads'</span>,<span class="string">'password'</span>:psd&#125;)</span><br><span class="line">    char = re.findall(<span class="string">'密码错误'</span>,html.text)</span><br><span class="line">    psd +=<span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'password ='</span>,psd<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><strong>最终密码：4</strong></p><hr><p><br><br></p><h2 id="第三关"><a href="#第三关" class="headerlink" title="第三关"></a>第三关</h2><h3 id="验证登录以及从爬虫保持Token访问"><a href="#验证登录以及从爬虫保持Token访问" class="headerlink" title="验证登录以及从爬虫保持Token访问"></a>验证登录以及从爬虫保持Token访问</h3><p><em>进入第三关的链接<a href="http://www.heibanke.com/lesson/crawler_ex02/" target="_blank" rel="noopener">http://www.heibanke.com/lesson/crawler_ex02/</a><br>但是跳转到了<code>login</code>登陆页面,一开始以为在线的题目出错了，但注册完在登陆进去之后，才明白题目确实是这样的要求：</em><br><img src="https://1j0ker.github.io/md_img/py_zj4.png" alt="exmple4.md"><br>注册登录后，发现题目：<br><img src="https://1j0ker.github.io/md_img/py_zj5.png" alt="exmple5.md"><br><br><br>和第二关很像，用第二关的代码改了下<code>url</code>在运行，出现了问题：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"summary"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">h1</span>&gt;</span>Forbidden <span class="tag">&lt;<span class="name">span</span>&gt;</span>(403)<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">p</span>&gt;</span>CSRF verification failed. Request aborted.<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br></pre></td></tr></table></figure><p>发现了访问拒绝，又是<strong>POST</strong>包，这时候要去看一看<strong>request</strong>里面post哪些东西？<br><img src="https://1j0ker.github.io/md_img/py_zj6.png" alt="exmple6.md"><br>很明显多了个<code>token值</code>，再仔细查看，是<strong>request</strong>里cookies的这个值<code>csrftoken=2m7tnWxAV9Bm9lfFjWQ3vvCxAvY3qSxe;</code><br><br><br>那现在问题是这个token值是怎么获得的？经过一番F12是访问题目页面时，<strong>respond</strong>里的<code>Set-Cookie: csrftoken=2m7tnWxAV9Bm9lfFjWQ3vvCxAvY3qSxe;</code>由服务器发送给我们，用来验证接下来我们操作的身份；<br><br><br>也就是说，我们要在<strong>POST</strong>数据时，首先爬取<code>csrftoken</code>值，然后将<code>token</code>一同传过去；</p><hr><p>py代码(这次就没用<code>re</code>模块了)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url_1 = <span class="string">'http://www.heibanke.com/accounts/login'</span></span><br><span class="line">url_2 = <span class="string">'http://www.heibanke.com/lesson/crawler_ex02/'</span></span><br><span class="line">s = requests.Session()  <span class="comment">#创建会话，保存token</span></span><br><span class="line"></span><br><span class="line">s.get(url_1)</span><br><span class="line">token_1 = s.cookies[<span class="string">'csrftoken'</span>]<span class="comment">#get登录，拿到当前token</span></span><br><span class="line">print(token_1)</span><br><span class="line">data_1 = &#123;</span><br><span class="line">    <span class="string">'username'</span>: <span class="string">'efre'</span>,<span class="comment">#注册过的账号</span></span><br><span class="line">    <span class="string">'password'</span>: <span class="string">'123456123456'</span>,</span><br><span class="line">    <span class="string">'csrfmiddlewaretoken'</span>: token_1,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">s.post(url_1,data=data_1)<span class="comment">#登录</span></span><br><span class="line"><span class="comment">#print(r.text)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_2 = &#123;</span><br><span class="line">    <span class="string">'username'</span>:<span class="string">'admin'</span>,</span><br><span class="line">    <span class="string">'password'</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">'csrfmiddlewaretoken'</span>:<span class="string">''</span>,</span><br><span class="line">    &#125;</span><br><span class="line">s.post(url_2)<span class="comment">#获取题目页面的初始token值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pwd <span class="keyword">in</span> range(<span class="number">31</span>):</span><br><span class="line">    token_2 = s.cookies[<span class="string">'csrftoken'</span>]</span><br><span class="line">    data_2[<span class="string">'csrfmiddlewaretoken'</span>]=token_2</span><br><span class="line">    data_2[<span class="string">'password'</span>]=pwd</span><br><span class="line">    print(pwd,<span class="string">'+'</span>,token_2)<span class="comment">#查看当前密码和当前token值</span></span><br><span class="line">    html = s.post(url_2, data=data_2)</span><br><span class="line">    <span class="comment">#print(html.text)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'密码错误'</span> <span class="keyword">not</span> <span class="keyword">in</span> html.text:</span><br><span class="line">        <span class="comment">#print(html.text)</span></span><br><span class="line">        print(<span class="string">'password ='</span>,pwd)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><strong>最终密码：21</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python爬虫闯关练习&quot;&gt;&lt;a href=&quot;#Python爬虫闯关练习&quot; class=&quot;headerlink&quot; title=&quot;Python爬虫闯关练习&quot;&gt;&lt;/a&gt;Python爬虫闯关练习&lt;/h1&gt;&lt;p&gt;黑板客爬虫闯关&lt;a href=&quot;http://www.hei
      
    
    </summary>
    
    
      <category term="To be continued..." scheme="http://yoursite.com/tags/To-be-continued/"/>
    
  </entry>
  
</feed>
